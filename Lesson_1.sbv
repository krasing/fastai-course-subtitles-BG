0:00:00.325,0:00:03.285
Здравейте и добре дошли в курса

0:00:03.285,0:00:06.305
"Многослойно машинно самообучение за програмисти", урок първи

0:00:06.305,0:00:07.505
Това е четвъртата

0:00:09.505,0:00:12.105
година, която го провеждаме,

0:00:12.560,0:00:15.720
но ще е много различна и много специална версия

0:00:16.220,0:00:18.960
поради ред причини

0:00:18.960,0:00:22.120
Първата причина да е различен е, че го провеждаме на живо

0:00:22.120,0:00:24.700
още от първия ден

0:00:24.700,0:00:28.500
на пълно или почти пълно затваряне на Сан Франциско

0:00:29.340,0:00:31.720
Ще го записваме през следващите два месеца

0:00:31.720,0:00:34.340
в разгара на тази глобална пандемия

0:00:34.580,0:00:37.880
Затова, ако нещата изглеждат малко откачени от време на време,

0:00:37.880,0:00:41.700
моля да ме извините, това е причината

0:00:42.500,0:00:47.460
Друга причина да е специален

0:00:49.220,0:00:53.440
е, че опитваме да го направим един вид окончателна версия.

0:00:53.980,0:00:57.000
След като го провеждаме открай време,

0:00:57.000,0:00:59.340
най-накрая стигнахме до точка, в която

0:00:59.340,0:01:01.320
като че ли знаем за какво говорим.

0:01:01.600,0:01:05.500
До ниво, в което Силвен и аз написахме истинска книга

0:01:05.900,0:01:08.775
и действащ софтуер

0:01:08.780,0:01:11.785
от нулата, наречен библиотека fastai, версия 2,

0:01:11.785,0:01:15.720
написахме рецензирана статия

0:01:16.120,0:01:18.120
за тази библиотека,

0:01:18.380,0:01:21.760
така че това е замислено да е версията на курса,

0:01:22.080,0:01:26.200
която, надяваме се, ще остане.

0:01:27.340,0:01:30.040
Учебната програма следва доста точно съдържанието

0:01:30.160,0:01:33.800
на книгата, така че, ако искате да

0:01:34.060,0:01:36.260
четете едновременно с провеждане на курса,

0:01:36.460,0:01:38.455
моля, купете я.

0:01:38.460,0:01:40.880
Казвам моля купете я, защото

0:01:41.120,0:01:43.660
цялото съдържание е достъпно и безплатно

0:01:43.665,0:01:46.525
под формата на Jupyter-тетрадки

0:01:46.525,0:01:49.800
и това е благодарение на голямата щедрост на

0:01:49.800,0:01:53.300
О'Рейли медии, които ни го позволиха.

0:01:54.220,0:02:00.020
Така че ще може да видите на сайта за курса,

0:02:00.260,0:02:03.000
как да получите достъп до всичко това,

0:02:05.380,0:02:07.280
тук е хранилището на fastbook,

0:02:07.580,0:02:10.640
където може да видите цялото проклето нещо.

0:02:11.920,0:02:17.300
В момента пише, че е чернова, но до момента, когато ще го видите,

0:02:17.580,0:02:21.040
вече няма да е, така че имаме голяма молба.

0:02:23.110,0:02:25.110
Сделката е следната -

0:02:25.300,0:02:29.580
може да четете това нещо безплатно под формата на Jupyter-тетрадки, но

0:02:30.000,0:02:35.900
така не е толкова удобно, както четенето на Киндл или, знаете, на хартиена книга

0:02:36.360,0:02:41.240
Затова моля не превръщайте материала в PDF. Моля, не го преобразувайте към

0:02:42.240,0:02:44.680
формат, който е специално за четене.

0:02:44.940,0:02:51.160
Защото идеята е, че се надяваме, че ще купите книгата. Не се

0:02:51.580,0:02:54.180
възползвайте от щедростта на O'Reilly,

0:02:54.540,0:02:56.019
като

0:02:56.019,0:02:58.019
създадете нещо, което

0:02:58.020,0:03:00.780
не ви се дава безплатно и

0:03:01.080,0:03:06.060
такъв е и изрично лицензът, по който ви осигуряваме достъпа.

0:03:07.140,0:03:09.660
Това е молба да сме свестни хора

0:03:10.060,0:03:15.120
Ако видите някой, който не е благоприличен и краде "книжната" версия на книгата,

0:03:15.120,0:03:20.100
моля кажете му "моля не правете така". Не е хубаво и не бъдете такъв човек.

0:03:21.000,0:03:26.960
Така или иначе, може да четете книгата съвместно с учебната програма.

0:03:28.480,0:03:34.140
Има няколко различни версии на тези тетрадки.

0:03:38.040,0:03:41.560
Има пълна версия, която

0:03:43.020,0:03:47.200
съдържа целия текст, картинки, всичко.

0:03:47.440,0:03:50.200
В действителност написахме

0:03:50.200,0:03:54.500
система, която преобразува тетрадките в печатна книга

0:03:54.740,0:03:58.500
и понякога това изглежда малко странно, например

0:03:58.500,0:04:03.480
ето странно изглеждаща таблица, но ако погледнете в истинската книга,

0:04:05.020,0:04:07.540
таблицата се вижда като нормална таблица.

0:04:07.720,0:04:11.459
Така че понякога ще виждате малки странни части/маркировки

0:04:11.459,0:04:17.939
Добре, те не са грешки. Те са части, където ние добавяме информация, за да помогнем на книгата ни да се превърне в истинска хубава книга.

0:04:17.940,0:04:19.940
Така че, просто ги игнорирайте.

0:04:21.070,0:04:24.239
Когато казвам "ние", кои сме ние?

0:04:25.570,0:04:28.749
Важна част от "ние" е

0:04:29.660,0:04:35.860
Силвер. Силвер ми е съавтор за книгата и за версия 2 на библиотеката fastai.

0:04:35.860,0:04:38.500
Така че той е моят съучастник в престъплението

0:04:39.180,0:04:41.080
Другият ключов

0:04:41.090,0:04:43.090
"ние" е

0:04:43.640,0:04:50.000
Рейчъл Томас. И така, Рейчъл, може да дойдеш и да се представиш. Тя е съосновател на fast.ai.

0:04:52.260,0:04:55.740
Здравейте, да, аз съм съосновател на fast.ai, аз съм също

0:04:56.780,0:05:04.630
по-ниска, извинете, по-висока от Джереми и съм основател на Центъра за приложна етика на Университета в Сан Франциско.

0:05:05.360,0:05:10.599
Наистина съм развълнувана да бъда част от този курс и ще бъда гласът, който ще чувате да задава въпроси от форумите.

0:05:15.050,0:05:20.979
Рейчъл и Силвер също така са хората от тази група, които всъщност разбират математиката. Аз съм завършил философия.

0:05:21.830,0:05:26.319
Рейчъл има докторска степен. Силвер е написала 10 книги по математика.

0:05:26.900,0:05:29.799
Ако възникнат въпроси по математика, може

0:05:30.320,0:05:32.320
да ги предам към тях.

0:05:32.330,0:05:37.059
Но е много хубаво да имаш възможност да работиш с хора, които разбират тази тема толкова добре.

0:05:38.360,0:05:40.419
Да. Да, Рейчъл. Искаш ли да

0:05:41.990,0:05:48.069
Разбира се, благодаря и както Рейчъл спомена и друга област, където тя

0:05:49.400,0:05:56.199
има експертиза от световна класа, е етиката на данните. Тя е учредителният директор на центъра за приложни данни

0:06:00.230,0:06:02.230
в университета на Сан Франциско. Благодаря

0:06:02.720,0:06:09.280
Ще говорим за етика на данните по време на целия курс, защото смятаме, че е много важно.

0:06:09.880,0:06:15.700
Така че за тези части, въпреки че по принцип аз ще ги представям, те ще бъдат като цяло базирани на

0:06:15.940,0:06:20.260
работата на Рейчъл, защото тя в действителност знае за какво говори,

0:06:21.460,0:06:24.460
въпреки че благодарение на нея и аз поназнайвам за какво говоря.

0:06:25.760,0:06:27.760
Точно така, така е това

0:06:31.040,0:06:37.420
Така че трябва ли да сте тук, има ли някакъв момент, който се опитвате да разберете?

0:06:40.549,0:06:42.549
Да разберете многослойното машинно самообучение (Deep Learning)

0:06:44.720,0:06:52.160
Така че какво правите тука, има ли смисъл да се опитвате да научите Deep Learning

0:06:52.400,0:07:01.000
или сте прекалено глупави, или нямате достатъчно големи ресурси или каквото и да е, защото това е каквото ни казват много хора.

0:07:01.000,0:07:06.360
Казват, че имате нужда от екипи от доктори и огромни центрове за данни, пълни с графични процесори. В противен случай,

0:07:06.889,0:07:08.889
иначе е безсмислено.

0:07:09.200,0:07:13.900
Не се притеснявайте. Това изобщо не е вярно. Даже не може да бъде по-далеч от истината.

0:07:14.320,0:07:19.940
Всъщност огромна част  изследвания от световна класа

0:07:20.180,0:07:26.380
и практически проекти на световно ниво са направени от завършили fast.ai,

0:07:27.700,0:07:33.940
проекти, базирани на библиотеката fast.ai и други,

0:07:33.940,0:07:37.240
създадени са на едно GPU,

0:07:37.460,0:07:42.060
използвайки няколко десетки или няколко стотици примерни записа

0:07:42.060,0:07:48.340
от хора, които нямат познания на студент, завършил техническа специалност,

0:07:48.340,0:07:53.860
или в моя случай нямам изобщо техническо образование. Аз съм завършил специалност философия.

0:07:54.619,0:07:56.599
Така че има,

0:07:56.599,0:07:58.089
както ще видим по време на курса,

0:07:58.089,0:08:03.099
има много и много и много ясни емпирични доказателства, че нямате нужда от много математика,

0:08:03.100,0:08:07.820
нямате нужда от много данни. Не ви трябват много скъпи компютри, за да правите страхотни неща с Deep Learning.

0:08:08.100,0:08:10.980
Така че, просто останете с нас. Всичко ще е наред.

0:08:11.780,0:08:14.140
За да се справите с този курс, трябва да може да програмирате.

0:08:15.460,0:08:17.460
За предпочитане е да знаете как да кодирате в Python,

0:08:18.259,0:08:19.989
но ако ползвате други езици,

0:08:19.989,0:08:21.989
може да научите Python.

0:08:22.069,0:08:28.208
Ако единствените езици, с които работите, са нещо като MATLAB, където сте ги използвали по-скоро като за скриптове за нещо,

0:08:28.939,0:08:31.629
ще ви бъде по-тежко,

0:08:32.990,0:08:37.419
но това е нормално. Може да учите Python в процеса на курса.

0:08:40.399,0:08:44.318
Има ли някакъв смисъл да учим за Deep Learning, има ли нещо полезно?

0:08:44.930,0:08:48.729
Ако се надявате да създадете мозък,

0:08:49.700,0:08:51.560
това е AGI.

0:08:51.580,0:08:55.600
Не мога да обещая, че ще ви помогна в това

0:08:55.600,0:09:00.120
и AGI означава "artificial general intelligence" изкуствен общ интелект. Благодаря

0:09:00.120,0:09:03.520
Това, което мога да ви кажа обаче, е че във всички тези области

0:09:04.130,0:09:10.900
Deep learning  е най-добрият известен подход, поне за много варианти на всички тези неща.

0:09:12.290,0:09:13.850
Така че

0:09:13.850,0:09:17.740
не е преувеличено, че това е полезен инструмент.

0:09:17.740,0:09:22.620
Това е полезен инструмент за много и много и много неща, изключително полезен инструмент.

0:09:22.620,0:09:26.280
И в много от тези случаи е еквивалентен

0:09:26.280,0:09:31.440
или по-добър от човешкото представяне, поне за определени тесни

0:09:31.820,0:09:35.400
определения за нещата, които хората правят в този тип области.

0:09:35.780,0:09:38.380
Така че deep learning е доста невероятно

0:09:38.390,0:09:39.260
и ако

0:09:39.260,0:09:45.760
Може да спрете видеото тук, да разгледате и да изберете нещо, което ви изглежда интересно

0:09:45.960,0:09:52.480
и да напишете ключовата дума и deep learning в Google и ще намерите много статии, примери и подобни неща.

0:09:54.860,0:09:56.390
Deep learning (многослойното машинно самообучение)

0:09:56.390,0:10:02.559
произлиза от невронните мрежи, както ще видите то е просто един вид

0:10:03.770,0:10:05.770
обучение на невронни мрежи

0:10:06.160,0:10:08.760
дълбоко - с много слоеве, по-нататък ще обясним точно какво означава това.

0:10:09.080,0:10:11.260
Невронните мрежи определено не са нещо ново.

0:10:11.560,0:10:15.240
Водят началото си от поне 1943 г., когато McCulloch и Pitt's

0:10:15.560,0:10:22.239
създават математичен модел на изкуствен неврон и много се вълнуват докъде може да ги доведе.

0:10:23.030,0:10:25.030
След това през 50-те

0:10:25.850,0:10:27.620
Франк Розенблат

0:10:27.620,0:10:29.620
построява на тази основа,

0:10:30.230,0:10:34.900
на практика прави минимални промени на този математичен модел

0:10:35.540,0:10:41.400
и смята, че с тези фини промени "можем да станем свидетели на раждането на машина, която е способна да възприема,

0:10:41.760,0:10:46.060
разпознава и идентифицира заобикалящата я среда без човешко обучение или контрол" и

0:10:46.640,0:10:48.940
надзирава изграждането на това

0:10:49.910,0:10:54.040
изключително нещо, mark 1 перцептрон, в Корнел.

0:10:56.270,0:10:58.749
Мисля, че тази снимка е от 1961.

0:11:00.050,0:11:06.820
За щастие в наши дни не е нужно да изграждаме невронни мрежи, като пускаме проводници от неврон към неврон,  изкуствен неврон,

0:11:07.070,0:11:09.309
но можете да видите идеята,

0:11:09.410,0:11:14.200
за какви връзки става дума и в този курс, ще чуете думата връзка много често, защото това е всъщност всичко.

0:11:16.570,0:11:20.109
След това имаме първата AI (Artificial Intelligence) зима, както е известна, която реално

0:11:20.840,0:11:25.299
до голяма степен се случва, защото професор от MIT на име Марвин Мински

0:11:26.000,0:11:33.520
и Паперт написват книга, наречена "Перцептрон" за изобретението на Розенблат, в която посочват, че един слой от

0:11:33.950,0:11:35.060
тези

0:11:35.060,0:11:37.060
устройства - изкуствени неврони,

0:11:37.280,0:11:39.080
всъщност не може да научи

0:11:39.080,0:11:44.979
някои критични неща, невъзможно е да научат нещо толкова просто като булевия оператор XOR.

0:11:46.340,0:11:51.309
В същата книга те показват, че използването на множество слоеве от устройствата всъщност ще отстрани проблема.

0:11:51.980,0:11:55.930
Хората игнорират, не забелязват тази част от книгата и забелязват само това

0:11:57.020,0:12:01.299
ограничение, и в общи линии решават, че невронните мрежи няма да отидат на никъде и

0:12:02.090,0:12:03.320
те

0:12:03.320,0:12:05.859
до голяма степен изчезват за десетилетия.

0:12:07.910,0:12:10.680
По някакъв начин през 1986 г., междувременно се случи много,

0:12:11.380,0:12:14.160
но през 1986 г. имаше голямо нещо:

0:12:14.900,0:12:21.309
MIT пусна книга, книга от два тома, наречена "Паралелна разпределена обработка" (PDP),

0:12:23.240,0:12:26.740
в която те описаха това, което наричат паралелна разпределена обработка,

0:12:27.320,0:12:32.559
където имате куп обработващи единици, които имат някакво състояние на

0:12:32.960,0:12:37.030
активиране и някаква изходна функция, и някакъв

0:12:37.340,0:12:43.689
модел на свързване, и някакво правило за разпространение и някакво правило на активиране, и някакво правило за учене, работещо в среда.

0:12:43.960,0:12:48.360
И тогава те описват как нещата, които отговарят на тези изисквания,

0:12:48.500,0:12:52.900
може на теория да вършат всякакви невероятни работи.

0:12:52.900,0:12:54.999
Това беше резултат от много, много

0:12:55.280,0:13:02.619
изследователи, работещи заедно като цяла група, участваща в този проект, в резултат на което се получи тази много важна книга.

0:13:03.080,0:13:09.909
Така че интересното тук за мен е, че ако вие, като преминете през този курс, се върнете и погледнете

0:13:10.280,0:13:18.219
тази снимка, ще видите, че правим точно тези неща, всичко, което учим, е как се прави

0:13:18.830,0:13:20.720
всяко едно от тези

0:13:20.720,0:13:25.959
осем неща. Интересно е, че те включват околната среда, защото това е нещо, което много често

0:13:26.720,0:13:31.509
учените по теория и анализ на данни пренебрегват. Това е, че ти изграждаш модел, обучаваш го, научил е нещо,

0:13:31.509,0:13:37.269
но в какъв контекст работи? Ще говорим за това доста и през следващите няколко урока.

0:13:39.619,0:13:41.619
Така през 80-те,

0:13:42.379,0:13:48.459
по време и след издаване на книгата, хората започнаха да слагат и този втори слой неврони,

0:13:48.859,0:13:52.329
избягвайки проблема на Мински, и всъщност

0:13:53.179,0:13:55.179
показано беше, 

0:13:55.879,0:14:02.079
математически беше доказано, че чрез добавяне на този допълнителен слой неврони

0:14:02.629,0:14:08.379
всеки математически модел може да се апроксимира с произволно ниво на точност с тези невронни мрежи.

0:14:09.319,0:14:13.599
И така това беше точно обратното на Мински.

0:14:13.600,0:14:18.850
Това беше като "ей, знаете, няма нещо, което да не можем да направим", доказуемо няма такова нещо.

0:14:19.260,0:14:22.660
И така бяха нещата, когато започнах да се занимавам с невронни мрежи.

0:14:23.060,0:14:27.020
Може би малко по-късно. Предполагам, че се включих в началото на

0:14:27.589,0:14:34.898
90-те години и те бяха много широко използвани в промишлеността. Използвах ги за много скучни неща, като целевия маркетинг за банките, обслужващи дребни клиенти.

0:14:35.989,0:14:41.829
Основно големи компании с много пари ги използваха и със сигурност беше вярно, че

0:14:42.289,0:14:49.239
често мрежите са били твърде големи или бавни, за да бъдат полезни. Със сигурност имаше полезни за нещо, но

0:14:50.829,0:14:57.758
никога не съм ги чувствал така, сякаш изпълняват обещаното по някаква причина. Това, което не знаех и 

0:14:58.549,0:15:00.230
никой, който лично съм срещал, не знаеше, е,

0:15:00.230,0:15:08.109
че е имало изследователи, които са показали преди 30 години, че за да се постигнат практически добри резултати, са нужни повече слоеве неврони.

0:15:08.809,0:15:16.509
Дори математически, теоретично да можете да получите такава точност, каквато искате, само с един допълнителен слой, за да го направите

0:15:16.970,0:15:24.579
с добри характеристики, се нуждаете от повече слоеве. Така че, когато добавите още слоеве към невронна мрежа, вие отивате на дълбоко (deep),

0:15:25.129,0:15:29.918
така че "дълбоко" (в deep learning) не означава нищо  мистично, а просто означава

0:15:30.439,0:15:34.189
повече слоеве. Повече слоеве от добавяне на един допълнителен.

0:15:35.850,0:15:42.200
Благодарение на това невронните мрежи сега достигат своя потенциал, както видяхме в какво е добро deep learning.

0:15:42.210,0:15:48.139
Така че сега можем да кажем, че Розенблат е бил прав. Ние имаме машина, която е способна

0:15:48.660,0:15:50.220
да възприема,

0:15:50.220,0:15:56.510
разпознава и идентифицира заобикалящата я среда без човешко обучение или контрол. Това определено е вярно.

0:15:56.510,0:16:00.229
Не мисля, че има нещо противоречиво в това твърдение на база на настоящата технология.

0:16:00.990,0:16:02.990
Така че ще се научим как да правим това.

0:16:04.939,0:16:09.290
Ще учим по точно обратния начин на вероятно цялото

0:16:09.290,0:16:13.770
ни обучение по математика и технически науки.

0:16:13.770,0:16:24.949
Няма да започнем с двучасов урок за сигмоидна функция или с изучаване на линейна

0:16:24.949,0:16:29.750
алгебра, или с курс по математически анализ.

0:16:29.750,0:16:37.709
И причината за това е, че хората, които изучават как да се преподава и учи, са открили, 

0:16:37.709,0:16:41.639
че това не е правилният подход за повечето хора.

0:16:41.639,0:16:50.019
Ние работим много на база на работата на професор Дейвид Перкинс от Харвард

0:16:50.019,0:16:56.740
и други, които работят по подобни въпроси, които говорят за идеята да се играе цялата

0:16:56.740,0:16:57.740
игра.

0:16:57.740,0:17:01.260
И така, да се играе цялата игра е като, базирайки се на аналогия със спорта, да

0:17:01.260,0:17:03.949
учиш някой на бейзбол.

0:17:03.949,0:17:10.791
Не ги вкарваш в класна стая и не започваш да ги учиш на физиката на

0:17:10.791,0:17:20.320
параболата и как да ударят топката, и история на 100 години бейзбол политика в три части,

0:17:20.320,0:17:24.290
след това 10 години по-късно им даваш да гледат игра

0:17:24.290,0:17:27.050
и накрая, 20 години по-късно, им даваш да играят играта.

0:17:27.050,0:17:31.930
Така се прави, един вид, при образованието по математика.

0:17:31.930,0:17:37.180
Вместо това при бейзбола първата стъпка е да се каже: хей, нека идем и погледаме малко бейзбол.

0:17:37.180,0:17:38.180
Какво мислите?

0:17:38.180,0:17:39.180
Забавно беше, нали?

0:17:39.180,0:17:42.060
Виж този играч там... той изтича натам... преди това другият хвърли топката ей натам... хей,

0:17:42.060,0:17:44.080
искаш ли да пробваш да направиш удар?

0:17:44.080,0:17:47.380
Добре, значи ти ще удариш топката и аз ще трябва да опитам да я хвана, след това той трябва

0:17:47.380,0:17:52.940
да тича, да тича натам... и така от първата стъпка вие играете цялата игра.

0:17:52.940,0:17:58.920
И само да добавя към това, че когато хората започват, те често може да нямат пълен отбор или да 

0:17:58.920,0:18:03.620
играят пълните девет подавания, но все пак имат усет какво е играта, нещо като

0:18:03.620,0:18:05.390
идея за голяма картина.

0:18:05.390,0:18:12.510
Така че има много причини, че това помага на повечето човешки създания, макар и

0:18:12.510,0:18:14.050
не на всеки.

0:18:14.050,0:18:19.430
Има малък процент хора, които предпочитат да изграждат нещата от фундамента и

0:18:19.430,0:18:24.270
от принципите, и не е изненадващо, че те преобладават в университетските

0:18:24.270,0:18:28.510
среди, защото хората, които стават учени, са хора, които виреят в 

0:18:28.510,0:18:33.350
преобърнатите (според мен) условия на преподаване на нещата.

0:18:33.350,0:18:40.450
Но извън университетите повечето хора учат най-добре по този начин, от горе на долу (от приложението към теорията), където

0:18:40.450,0:18:42.390
започваш с пълния контекст.

0:18:42.390,0:18:47.290
Стъпка номер две от седемте принципа, от които ще спомена само първите три,

0:18:47.290,0:18:49.620
е да се направи играта да си струва да се играе.

0:18:49.620,0:18:53.590
Това е като, ако играеш бейзбол, да имаш състезание.

0:18:53.590,0:19:00.140
Знаете, трупате точки, опитвате да печелите, събирате отбори от цялата общност

0:19:00.140,0:19:02.350
и хората опитват да се победят един друг.

0:19:02.350,0:19:08.640
Имате табло с победителите, кой има най-висок резултат и подобни. 

0:19:08.640,0:19:14.800
Всичко това е, за да сме сигурни, че каквото правим, го правим правилно.

0:19:14.800,0:19:23.000
Правим цялото нещо, даваме контекста и създаваме интерес.

0:19:23.000,0:19:30.490
И така, при подхода на fastai за изучаване на deep learning това означава, че днес ще

0:19:30.490,0:19:33.200
обучаваме модели от начало до край.

0:19:33.200,0:19:38.350
Ние реално ще обучим модели и те няма да са скапани модели.

0:19:38.350,0:19:45.070
Ще бъдат съвременни модели на световно ниво и ние ще се опитаме да ви накараме

0:19:45.070,0:19:50.910
да създадете собствени съвременни модели още днес или при следващия урок, според това

0:19:50.910,0:19:52.810
как ще се развият нещата.

0:19:52.810,0:19:59.200
Номер три от седемте принципа от Харвард е да се работи върху трудните части.

0:19:59.200,0:20:10.050
Това е като идеята за практика, целенасочена практика.

0:20:10.050,0:20:19.490
Работа върху трудната част означава, че вие не просто полюшвате бухалката към топката, знаете,

0:20:19.490,0:20:22.340
когато излизате да се размотавате.

0:20:22.340,0:20:27.460
Вие тренирате както трябва, намирате тази част, в която сте най-слаби, установявате

0:20:27.460,0:20:31.340
къде е проблемът и работите дяволски упорито върху него.

0:20:31.340,0:20:39.530
В контекста на дълбокото машинно самообучение това означава, че нищо няма да претупваме.

0:20:39.530,0:20:40.530
Така ли е?

0:20:40.530,0:20:45.210
До края на този курс вие вече ще сте ползвали висша математика (математически анализ).

0:20:45.210,0:20:47.400
Ще сте ползвали линейна алгебра.

0:20:47.400,0:20:54.380
Ще сте правили софтуерно инженерство на програмен код.

0:20:54.380,0:21:04.290
Ще практикувате тези неща, които са трудни, така че е необходима упоритост и ангажираност.

0:21:04.290,0:21:11.290
Но да се надяваме, ще разберете защо това има значение, защото преди да започнете да практикувате нещо,

0:21:11.290,0:21:14.470
вие вече ще знаете защо ви е необходимо, защото вече ще го ползвате.

0:21:14.470,0:21:19.450
Например, за да направите модела си по-добър, ще трябва да разберете концепцията му.

0:21:19.450,0:21:24.490
За тези от вас, които са свикнали на традиционната университетска среда, това ще изглежда

0:21:24.490,0:21:31.190
доста странно и много хора казват, че съжаляват (след година изучаване на fastai),

0:21:31.190,0:21:37.560
че са прекарали прекалено много време, изучавайки теория, и недостатъчно време за обучаване на модели

0:21:37.560,0:21:39.500
и писане на код.

0:21:39.500,0:21:43.250
Това е основната обратна връзка, която получаваме от хората, които казват:

0:21:43.250,0:21:44.570
"Бих искал да бях направил нещата различно."

0:21:44.570,0:21:45.790
Това е.

0:21:45.790,0:21:53.120
Затова моля, опитайте колкото можете повече, след като сте тук, да следвате този подход.

0:21:53.120,0:21:59.400
Ще ползваме комплект програмни средства... Извинявай, Рейчъл.

0:21:59.400,0:22:00.400
Да?

0:22:00.400,0:22:02.510
Трябва да кажа още едно нещо за този подход.

0:22:02.510,0:22:07.390
Мисля, че след като много от нас са прекарали толкова години с традиционния подход за обучение,

0:22:07.390,0:22:12.030
от основите към приложението (bottom-up), промяната може да се усеща в началото много некомфортно.

0:22:12.030,0:22:16.620
Аз все още се чувствам некомфортно с това от време на време, въпреки че съм отдадена на идеята.

0:22:16.620,0:22:21.850
Част от това е, че трябва да се удържите и да сте добре и без да знаете 

0:22:21.850,0:22:22.850
подробностите.

0:22:22.850,0:22:28.190
Което според мен може да се почувства много непознато или дори погрешно, когато си нов в това.

0:22:28.190,0:22:31.930
Подобно на: „О, почакай, използвам нещо и не разбирам всяка подробност в основата му."

0:22:31.930,0:22:36.370
Но трябва да се доверите, че ще стигнем до тези подробности по-късно.

0:22:36.370,0:22:39.560
Така че не мога да съчувствам, защото не прекарах много време в това.

0:22:39.560,0:22:44.050
Но ще ви кажа - преподаването по този начин е много, много, много трудно.

0:22:44.050,0:22:49.430
И често се хващам, че прескачам обратно към подхода "първо основите".

0:22:49.430,0:22:51.810
Защото е толкова лесно да кажеш: "О, 

0:22:51.810,0:22:52.810
трябва да знаеш това...

0:22:52.810,0:22:53.810
трябва да знаеш това...

0:22:53.810,0:22:55.140
и тогава можеш да знаеш това."

0:22:55.140,0:22:56.540
Така е много по-лесно да се преподава.

0:22:56.540,0:23:01.760
Така че намирам това много по-предизвикателно за преподаване, но да се надяваме, че си заслужава.

0:23:01.760,0:23:06.980
Прекарахме дълго време да измислим как да представим deep learning в този формат,

0:23:06.980,0:23:11.620
но едно от нещата, което ни помогна, е софтуерът, с който разполагаме.

0:23:11.620,0:23:23.010
Ако не сте използвали Python преди - това е изключително гъвкав, изразителен и лесен за използване език.

0:23:23.010,0:23:28.210
Има много части от него, които не харесваме, но като цяло обичаме цялостното нещо.

0:23:28.210,0:23:33.790
И ние мислим, и по-важното е, че болшинството от изследователите и  практикуващите 

0:23:33.790,0:23:38.000
deep learning използват Python.

0:23:38.000,0:23:42.680
Над Python има две библиотеки, които повечето колеги използват днес: PyTorch и

0:23:42.680,0:23:44.140
TensorFlow.

0:23:44.140,0:23:47.550
Тук се случи много бърза промяна.

0:23:47.550,0:23:51.090
Допреди няколко години преподавахме TensorFlow.

0:23:51.090,0:23:54.970
Тогава го използваха всички.

0:23:54.970,0:24:00.370
Най-общо TensorFlow затъна много.

0:24:00.370,0:24:05.930
Дойде и този друг софтуер, наречен PyTorch, който беше много по-лесен за използване и много по-

0:24:05.930,0:24:15.040
полезен за изследователите и през последните 12 месеца, процентът на статиите на големи

0:24:15.040,0:24:21.290
конференции, които използват PyTorch, е нараснал от 20% на 80% и обратно, тези, които използват

0:24:21.290,0:24:24.960
TensorFlow, е намалял от 80% на 20%.

0:24:24.960,0:24:29.030
Така че всички хора, които всъщност изграждат технологията, всички сега използват 

0:24:29.030,0:24:35.340
PyTorch, и знаете, индустрията се движи малко по-бавно, но през следващите година-две

0:24:35.340,0:24:38.900
вероятно ще сте свидетели на подобно нещо и в индустрията.

0:24:38.900,0:24:45.180
Особеното на PyTorch е, че е супер, супер гъвкав и наистина е създаден 

0:24:45.180,0:24:52.570
за гъвкавост и дружелюбност за разработчици, със сигурност не е създаден за дружелюбност към начинаещите и

0:24:52.570,0:24:57.950
не е проектиран за, както казваме, няма API на високо ниво, под което разбирам,

0:24:57.950,0:25:05.630
че няма неща, които да улесняват бързото изграждане на неща с помощта на PyTorch.

0:25:05.630,0:25:13.700
За да се справим с този проблем, имаме библиотека, наречена fastai, която стъпва върху PyTorch.

0:25:13.700,0:25:20.330
fastai е най-популярният API на по-високо ниво за PyTorch.

0:25:20.330,0:25:27.520
Тъй като нашите курсове са толкова популярни, някои хора са с погрешното впечатление,

0:25:27.520,0:25:34.260
че fastai е създаден за начинаещи или за преподаване.

0:25:34.260,0:25:43.840
Създаден е за начинаещи и за преподаване, както и за практикуващи от индустрията и за изследователи.

0:25:43.840,0:25:50.290
Начинът, по който постигаме това, гарантира, че това е най-добрият API за всички тези хора,

0:25:50.290,0:25:59.340
тъй като използваме нещо, наречено многослоен API, и има рецензирана статия, която Силвен

0:25:59.340,0:26:04.600
и аз написахме, която описва как сме го направили и за тези от вас, които са софтуерни инженери,

0:26:04.600,0:26:08.130
това няма да бъде нещо необичайно или изненадващо.

0:26:08.130,0:26:12.590
Това са просто напълно стандартни практики от софтуерното инженерство, но практики, които

0:26:12.590,0:26:17.030
не са следвани в нито една позната ни библиотека за deep learning.

0:26:17.030,0:26:24.500
В основни линии много ре-факторинг и разединяване и така този подход ни позволи

0:26:24.500,0:26:32.410
да изградим нещо, което може да прави изследвания от съвсем ниско ниво, може да правите съвременни

0:26:32.410,0:26:43.850
модели за внедряване и може да правите много лесно, за начинаещи, модели за начинаещи, но на световно ниво.

0:26:43.850,0:26:47.220
Това е основният софтуерен стек, има и други части софтуер, за които ще учим

0:26:47.220,0:26:49.940
по пътя ни.

0:26:49.940,0:26:54.570
Но основното, което мисля да спомена тук, е, че всъщност няма значение.

0:26:54.570,0:27:01.300
Ако овладеете този софтуерен стек и след това на работа трябва да използвате TensorFlow и Keras,

0:27:01.300,0:27:06.380
ще може да превключите за по-малко от седмица.

0:27:06.380,0:27:13.070
Много и много студенти са го правили и никога не е било проблем.

0:27:13.070,0:27:20.700
Важното е да се научи концепцията и затова ще се фокусираме върху тези концепции

0:27:20.700,0:27:27.870
и като използваме API, което свежда до минимум количеството на помощните работи, които трябва да използвате, ще 

0:27:27.870,0:27:29.730
може да се фокусирате върху елементите, които са важни.

0:27:29.730,0:27:38.150
Конкретните редове код ще отговарят много повече на конкретните концепции, които прилагате.

0:27:38.150,0:27:41.450
Ще ви трябва GPU машина.

0:27:41.450,0:27:49.420
GPU е графичен процесор и по-специално се нуждаете от Nvidia GPU.

0:27:49.420,0:27:55.520
Други марки графични процесори просто не се поддържат добре от никакви библиотеки за Deep Learning.

0:27:55.520,0:27:56.970
Моля, не купувайте такъв.

0:27:56.970,0:28:00.130
Ако вече имате такъв, вероятно не трябва да го използвате.

0:28:00.130,0:28:05.400
Вместо това трябва да използвате една от платформите, които вече сме настроили за вас.

0:28:05.400,0:28:10.410
Просто е огромно разсейване да прекарвате времето си в системна администрация

0:28:10.410,0:28:16.330
на GPU машина, инсталирайки драйвери и т.н.

0:28:16.330,0:28:18.000
И го стартирайте на Линукс.

0:28:18.000,0:28:19.000
Моля ви!

0:28:19.000,0:28:22.260
Така правят всички, не само ние, всеки изпълнява кода на Линукс.

0:28:22.260,0:28:23.370
Направете живота си лесен.

0:28:23.370,0:28:28.200
Достатъчно трудно е да изучавате Deep Learning и без да се налага да си добавяте допълнителни пречки,

0:28:28.200,0:28:31.550
знаете всякакви тайнствени проблеми с хардуерната поддръжка.

0:28:31.550,0:28:43.480
Има много безплатни варианти на разположение, затова моля, използвайте ги.

0:28:43.480,0:28:48.280
Ако използвате вариант, който не е безплатен, не забравяйте да изключите виртуалната машина.

0:28:48.280,0:28:51.730
Това, което ще стане, е, че ще държите сървър, който работи някъде

0:28:51.730,0:28:57.260
другаде по света, и ще се свързвате към него от своя компютър, и ще обучавате,

0:28:57.260,0:29:01.500
пускате и създавате модели.

0:29:01.500,0:29:05.980
Това, че затваряте прозореца на браузъра си, не означава, че сървърът ви спира да работи

0:29:05.980,0:29:06.980
 като цяло.

0:29:06.980,0:29:07.980
Разбрахте ли?

0:29:07.980,0:29:11.330
Затова не забравяйте да го изключите, защото в противен случай плащате за него.

0:29:11.330,0:29:16.120
Google Colab е страхотна система, която е безплатна.

0:29:16.120,0:29:18.650
Също така има и версия за платен абонамент.

0:29:18.650,0:29:21.200
Бъдете внимателни с Колаб.

0:29:21.200,0:29:26.590
Повечето от останалите системи, които препоръчваме, запазват работата ви автоматично и може

0:29:26.590,0:29:28.460
да се върнете и да продължите по всяко време.

0:29:28.460,0:29:29.460
Colab не.

0:29:29.460,0:29:37.110
Затова не забравяйте да проверите във форумите темата за платформата Colab, за да научите повече

0:29:37.110,0:29:44.020
Като споменах форумите...

0:29:44.020,0:29:51.970
Форумите са наистина много важни, защото там са всички дискусии, уговорки

0:29:51.970,0:29:54.030
и всичко, което се случва.

0:29:54.030,0:29:56.240
Така например, ако искате помощ при настройката,

0:29:56.240,0:30:03.820
знаете, има помощна тема за настройка и можете да разберете как най-добре да настроите 

0:30:03.820,0:30:09.150
Colab, и можете да видите дискусии за него, и да задавате въпроси. Моля, не забравяйте

0:30:09.150,0:30:12.310
да търсите, преди да задавате въпроси, нали?

0:30:12.310,0:30:18.620
Защото вероятно е попитано и преди, освен ако не сте един от първите хора, 

0:30:18.620,0:30:22.530
които правят курса.

0:30:22.530,0:30:24.820
И така...

0:30:24.820,0:30:31.570
И така, първата стъпка е да настроите вашия сървър, като просто следвате инструкциите от

0:30:31.570,0:30:33.620
форумите или от уебсайта на курса.

0:30:33.620,0:30:39.880
И уебсайтът на курса ще има много инструкции стъпка по стъпка за всяка платформа.

0:30:39.880,0:30:44.680
Те ще варират в цената, ще се различават по скорост, наличност и

0:30:44.680,0:30:46.390
така нататък.

0:30:46.390,0:30:48.390
След като приключите с прилагане на инструкциите,

0:30:48.390,0:30:57.790
като последна стъпка от инструкциите ще видите нещо такова: 

0:30:57.790,0:31:01.290
папка course v4, т.е. версия четвърта на курса.

0:31:01.290,0:31:04.820
До момента, в който гледате това видео, вероятно ще има още неща  в нея, но ще има

0:31:04.820,0:31:07.670
NB - означаващо папка с тетрадки (notebooks).

0:31:07.670,0:31:15.260
Така че може да цъкнете върху това и ще видите всички тетрадки за курса.

0:31:15.260,0:31:21.060
Това, което искам да направите, е да превъртите до долу и да намерите тази, наречена app Jupyter.

0:31:21.060,0:31:27.760
Цъкнете и там може да започнете да учите за Jupyter тетрадките.

0:31:27.760,0:31:30.220
Какво е Jupyter тетрадка?

0:31:30.220,0:31:39.630
Jupyter тетрадка е нещо, където може да пишете неща и да натиснете Shift-Enter,

0:31:39.630,0:31:41.250
и ще ви даде отговор.

0:31:41.250,0:31:47.440
Нещото, което пишете, е код на Python и това, което получавате, е резултатът

0:31:47.440,0:31:48.780
от изпълняване на кода.

0:31:48.780,0:31:52.490
И така, може да сложите каквото и да е на Python.

0:31:52.490,0:31:56.620
X е равно на три по четири

0:31:56.620,0:32:05.240
X плюс едно, и както може да видите, дава ви резултат всеки път, когато има резултат за показване.

0:32:05.240,0:32:10.290
Тези от вас, които сте се занимавали малко с програмиране, ще разпознаете

0:32:10.290,0:32:11.490
REPL

0:32:11.490,0:32:16.370
R-E-P-L, read, evaluate, print, loop - прочети, изчисли, отпечатай, цикъл.

0:32:16.370,0:32:18.940
Повечето езици имат някакъв вид REPL.

0:32:18.940,0:32:29.320
REPL на Jupyter тетрадките е особено интересен, тъй като има неща като заглавия,

0:32:29.320,0:32:34.850
графични изходи, интерактивна мултимедия.

0:32:34.850,0:32:37.680
Това е наистина изумителен софтуер.

0:32:37.680,0:32:39.850
Печелил е няколко наистина големи награди.

0:32:39.850,0:32:48.200
Бих си помислил, че е най-широко използваният REPL, извън командните обвивки (shell) като bash.

0:32:48.200,0:32:50.280
Това е много мощна система.

0:32:50.280,0:32:51.330
Ние я обожаваме.

0:32:51.330,0:32:55.940
Написахме цялата си книга в нея, написахме цялата библиотека fastai с нея,

0:32:55.940,0:32:58.970
правим цялото си обучение с нея.

0:32:58.970,0:33:06.780
Това е крайно непознато за хората, които са извършвали по-голямата част от работата си в IDE.

0:33:06.780,0:33:11.070
Трябва да очаквате, че ще се почувствате толкова неудобно, колкото може би първия път, когато сте преминали от GUI

0:33:11.070,0:33:13.200
към команден ред.

0:33:13.200,0:33:14.200
Различно е.

0:33:14.200,0:33:22.080
Така че, ако не сте запознати със системи, базирани на REPL, ви е супер странно.

0:33:22.080,0:33:25.170
Но се придържайте към нея, защото наистина е страхотно.

0:33:25.170,0:33:31.980
Моделът, който се случва тук, е, че тази уеб страница, която разглеждам, ми позволява да въвеждам

0:33:31.980,0:33:37.460
неща, които сървърът да изпълни, и ми показва резултатите от изчисленията, които прави сървърът.

0:33:37.460,0:33:40.170
Значи сървърът е някъде другаде.

0:33:40.170,0:33:42.650
Не работи на моя компютър, ясно?

0:33:42.650,0:33:45.810
Единственото, което работи на компютъра, е тази уеб страница.

0:33:45.810,0:33:53.360
Но като правя нещата, така например, ако кажа, че X е равно на X по три,

0:33:53.360,0:33:55.930
това актуализира състоянието на сървъра.

0:33:55.930,0:33:56.950
Има това състояние.

0:33:56.950,0:34:02.530
Това е каква в момента е стойността на X и така мога да разбера. Сега X е нещо

0:34:02.530,0:34:03.530
различно.

0:34:03.530,0:34:09.669
Така че можете да видите, когато направих този ред тук, той не промени по-ранния X плюс един, нали?

0:34:09.669,0:34:14.730
Това означава, че когато гледате Jupyter тетрадка, тя не ви показва текущото

0:34:14.730,0:34:16.559
състояние на сървъра.

0:34:16.559,0:34:21.679
Просто ви показва какво е било това състояние в момента, когато сте го отпечатали.

0:34:21.679,0:34:24.889
Това е все едно използвате shell (команден ред) като bash.

0:34:24.889,0:34:26.780
Въвеждате "ls",

0:34:26.780,0:34:28.629
след това изтривате файл,

0:34:28.629,0:34:31.990
предишното "ls", което сте отпечатали, не се променя.

0:34:31.990,0:34:35.559
Така най-общо работят REPL,

0:34:35.559,0:34:38.679
включително и този.

0:34:38.679,0:34:43.280
Jupyter тетрадките имат два режима.

0:34:43.280,0:34:48.880
Единият е режим "редактиране", който е, когато цъкна върху клетка и получа мигащ курсор и мога 

0:34:48.880,0:34:52.190
да се местя наляво и надясно и да въвеждам текст.

0:34:52.190,0:34:53.389
Така ли е?

0:34:53.389,0:34:55.740
Няма много клавишни комбинации към този режим.

0:34:55.740,0:35:01.609
Една полезна е "Control" или "Command" + "/" за превръщане в коментар или обратно.

0:35:01.609,0:35:07.509
Основната клавишна комбинация е “shift” + “enter” за изпълнение на клетката.

0:35:07.509,0:35:09.950
В този момент вече няма мигащ курсор.

0:35:09.950,0:35:12.319
Това означава, че вече съм в "команден" режим,

0:35:12.319,0:35:13.640
а не режим на редактиране.

0:35:13.640,0:35:17.460
Като отивам нагоре и надолу, избирам различни клетки,

0:35:17.460,0:35:23.480
така в команден режим, като се местим, избираме клетки

0:35:23.480,0:35:26.829
и вече има много клавишни комбинации, които можете да използвате.

0:35:26.829,0:35:30.600
Ако натиснете „H“, можете да получите списък с тях.

0:35:30.600,0:35:36.150
И ще видите, че не са като цяло от типа "Control" или "Command"

0:35:36.150,0:35:38.049
с нещо друго, а са просто отделни букви.

0:35:38.049,0:35:41.799
Ако сте използвали Vim, ще сте по-запознати с тази идея.

0:35:41.799,0:35:45.680
Например, ако натиснете "C" да копирам и "V" да поставя

0:35:45.680,0:35:47.440
и клетката се копира.

0:35:47.440,0:35:50.769
Или "X", за да я отрежа.

0:35:50.769,0:35:55.150
"A" добавя нова клетка отгоре на текущата

0:35:55.150,0:35:58.780
и след това мога да натисна различни цифри, за да създам заглавия,

0:35:58.780,0:36:01.890
например клавиш две ще направи заглавие от второ ниво.

0:36:01.890,0:36:08.319
И както виждате, мога да въвеждам форматиран текст, не просто код.

0:36:08.319,0:36:15.569
Форматираният текст, който въвеждам е във формат Markdown.

0:36:15.569,0:36:22.990
като това

0:36:22.990,0:36:24.369


0:36:24.369,0:36:26.800


0:36:26.800,0:36:28.880
Така че това е в Markdown

0:36:28.880,0:36:34.950
Ако досега не сте използвали Markdown, това е супер супер полезен начин за писане на форматиран

0:36:34.950,0:36:35.950
текст

0:36:35.950,0:36:38.359
Използва се много, много широко

0:36:38.359,0:36:41.760
Научете го, защото е много удобен

0:36:41.760,0:36:46.390
и ще ви трябва за Jupyter

0:36:46.390,0:36:51.930
Като погледнете в тетрадките на нашата книга

0:36:51.930,0:36:57.839
може да видите примери за всякакви видове форматиране и код и неща

0:36:58.839,0:37:05.240
Така че трябва да продължите и да преминете през „app_jupyter“

0:37:05.240,0:37:08.869
и може да видите например, как да създадете графики

0:37:08.869,0:37:10.900
и списъци с неща

0:37:10.900,0:37:12.869
да въвеждате библиотеки

0:37:12.869,0:37:18.430
да показвате картинки и т.н.

0:37:18.430,0:37:25.740
Ако искате да създадете нова тетрадка, може просто да идете на "New" "Python 3" и това

0:37:25.740,0:37:29.940
създава нова тетрадка

0:37:29.940,0:37:35.450
Която по подразбиране е наименувана “Untitled”, така че може да я преименувате за да и дадете каквото

0:37:35.450,0:37:38.460
име си харесате

0:37:38.460,0:37:45.480
И ще видите новото име, в списъка тук, "newname".

0:37:45.480,0:37:50.269
Другото, което трябва да знаете за Jupyter е, че е хубав лесен начин да скочите в терминал

0:37:50.269,0:37:51.450
ако знаете, как да използвате терминал

0:37:51.450,0:37:54.509
Разбира се не ви е нужно за този курс или поне за първите неща

0:37:54.509,0:38:08.269
Ако натиснете нов терминал ... и може да видите, че имам терминал

0:38:08.269,0:38:19.140
Едно нещо, което трябва да се отбележи е, че тетрадките са прикрепени към хранилище на Github

0:38:19.140,0:38:21.950
Ако не сте използвали Github преди, всичко е наред,

0:38:21.950,0:38:27.290
но по принцип те са прикачени към сървър, където от време на време ще актуализираме

0:38:27.290,0:38:29.480
тетрадките

0:38:29.480,0:38:33.329
Ще видите на уебсайта на курса, във форума, ще ви кажем как да направите така,

0:38:33.329,0:38:35.890
че да имате най-новите версии.

0:38:35.890,0:38:40.640
Като вземете най-новите версии, не искате да влязат в конфликт или да презапишат вашите

0:38:40.640,0:38:41.640
промени.

0:38:41.640,0:38:50.329
Затова, като започнете да експериментирате, не е лоша идея да изберете тетрадка и да цъкнете дублирай (duplicate),

0:38:50.329,0:38:52.500
и да започнете работата си върху копието.

0:38:52.500,0:38:59.099
Така, когато получите актуализация на последните материали от курса, те няма да развалят

0:38:59.099,0:39:06.089
експериментите, които сте изпълнявали.

0:39:06.089,0:39:09.490
Има две важни хранилища, за които трябва да знаете.

0:39:09.490,0:39:20.089
Едното е хранилището fastbook, което видяхме по-рано, което е нещо като пълната книга

0:39:20.089,0:39:26.099
с всички резултати и проза и всичко.

0:39:26.099,0:39:30.390
И другото е хранилището course-v4

0:39:30.390,0:39:34.690
и тук е съвсем същата тетрадка,

0:39:34.690,0:39:42.130
като за тази сме премахнали целия текст и всички картинки, и всички резултати,

0:39:42.130,0:39:46.210
и сме оставили само заглавията и кода.

0:39:46.210,0:39:51.029
В този случай може да видите някои резултати, защото току-що изпълних кода, но за по-голямата част

0:39:51.029,0:39:53.410
няма да има никакви.

0:39:53.410,0:39:56.300
Не, не, предполагам, че сме оставили резултати.

0:39:56.300,0:39:58.009
Не съм сигурен дали да ги запазя, или не.

0:39:58.009,0:40:01.849
Така че може да видите или да не видите резултати.

0:40:01.849,0:40:04.259
Главната идея е

0:40:04.259,0:40:10.480
това да е версията, с която да искате да експериментирате,

0:40:10.480,0:40:15.170
защото един вид ви кара да мислите, например, какво ще стане, като изпълнявате всяка стъпка,

0:40:15.170,0:40:18.799
вместо просто да четете и да изпълнявате, без да мислите.

0:40:18.799,0:40:24.059
Искаме да го правите в малко по-гола среда, в която мислите,

0:40:24.059,0:40:28.859
какво казва книгата, защо това става така и ако забравите нещо, тогава

0:40:28.859,0:40:31.321
може да се върнете обратно към книгата.

0:40:31.321,0:40:36.910
Другото, което трябва да се спомене, е, че както course-v4 версията, така и fastbook версията

0:40:36.910,0:40:42.000
имат въпросник.

0:40:42.000,0:40:46.529
И доста хора ни казаха по време на прегледи и подобни, че

0:40:46.529,0:40:49.990
всъщност първо четат въпросника.

0:40:49.990,0:40:57.869
Прекарахме много, много седмици в изготвяне на въпросника, Силвен и аз.

0:40:57.869,0:41:04.380
И причината за това е, че се опитваме да измислим какво искаме от вас

0:41:04.380,0:41:07.680
да усвоите от всяка тетрадка.

0:41:07.680,0:41:10.029
Така че може първо да прочетете въпросника.

0:41:10.029,0:41:12.150
Да видите какви са нещата, които според нас са важни.

0:41:12.150,0:41:14.940
Какви са нещата, които трябва да знаете, преди да продължите.

0:41:14.940,0:41:18.859
Така че вместо да имате обобщен раздел в края, който казва "накрая трябва да

0:41:18.859,0:41:24.920
знаете... дъра-бъра...", вместо това имаме въпросник, който прави същото, така че моля постарайте се

0:41:24.920,0:41:27.730
да направите въпросника, преди да се прехвърлите на следващата глава.

0:41:27.730,0:41:31.600
Не е нужно да разберете всичко правилно и в повечето случаи да отговорите на въпросите е

0:41:31.600,0:41:37.999
толкова просто, колкото да се върнете в тази част на тетрадката и да прочетете текста, но ако сте 

0:41:37.999,0:41:43.289
изпуснали нещо, върнете се назад и го прочетете, защото това са нещата, които приемаме, 

0:41:43.289,0:41:44.960
че ще знаете.

0:41:44.960,0:41:50.420
Така че, ако не знаете тези неща, преди да продължите, това може да доведе до разочарование.

0:41:50.420,0:41:57.499
След тези уговорки, ако все пак блокирате, след няколко опита продължете към следващата глава,

0:41:57.499,0:42:00.809
направете две, три или повече глави и се върнете обратно

0:42:00.809,0:42:04.480
може би до момента, в който сте минали още няколко глави, ще получите малко повече

0:42:04.480,0:42:05.480
перспектива.

0:42:05.480,0:42:13.099
Опитваме се да дообясняваме нещата по няколко пъти по различни начини, така че всичко е наред, когато сте опитали

0:42:13.099,0:42:17.329
и сте се закучили, тогава може да опитате да продължите.

0:42:17.329,0:42:26.410
И така, нека опитаме да пуснем първата част на тетрадката.

0:42:26.410,0:42:37.490
Намираме се в 01_intro, това е глава 1 и това е първата клетка.

0:42:37.490,0:42:45.481
Така че кликвам върху клетката и... по подразбиране всъщност ще има заглавка в лентата с инструменти,

0:42:45.481,0:42:46.481
както може да видите,

0:42:46.481,0:42:47.481
може да я показвате или скривате.

0:42:47.481,0:42:53.640
Аз лично винаги ги оставям скрити и така, за да изпълните клетката, трябва или да цъкнете на Play,

0:42:53.640,0:42:56.940
бутонът за пускане, или както споменах, да натиснете клавиши Shift-Enter.

0:42:56.940,0:43:03.349
В случая просто ще цъкна и, както виждате, появява се тази звездичка, което ни казва

0:43:03.349,0:43:07.880
"Аз работя" и може да видите новоизскочилата лента за напредък. И това ще отнеме

0:43:07.880,0:43:15.680
няколко секунди, и докато работи, ще отпечата някакви резултати.

0:43:15.680,0:43:20.740
Не очаквайте да получите съвсем същите резултати като нашите, има определена случайност, свързана с

0:43:20.740,0:43:23.779
обучаването на модел, и това е нормално.

0:43:23.779,0:43:26.530
Не очаквайте да получите и съвсем същото време като нас.

0:43:26.530,0:43:32.510
Ако първата клетка отнеме повече от пет минути, освен ако нямате наистина старо GPU, това вероятно е

0:43:32.510,0:43:33.510
лош знак.

0:43:33.510,0:43:38.609
Може да се прехвърлите към форумите и да потърсите какво не е наред или може би

0:43:38.609,0:43:43.400
се опитвате да използвате Windows, който наистина не работи особено добре с това към момента.

0:43:43.400,0:43:45.210
Не се притеснявайте, че не знаем какво прави кодът все още.

0:43:45.210,0:43:52.410
Просто се убеждаваме, че можем да обучим модел. И така, ето ни, свърши да работи

0:43:52.410,0:43:58.079
и както може да видите, отпечата се някаква информация, в този случай ми показва,

0:43:58.079,0:44:05.269
че има грешка (error rate) 0.005 при правенето на нещо.

0:44:05.269,0:44:06.569
Какво е нещото, което прави?

0:44:06.569,0:44:14.089
Това, което прави, е, че взема набор от данни (dataset), извикваме данните за домашни любимци,

0:44:14.089,0:44:18.849
което е набор от снимки на котки и кучета.

0:44:18.849,0:44:25.829
И се опитва да разбере: кои са котки и кои са кучета.

0:44:25.829,0:44:32.599
И както можете да видите, след около по-малко от минута той е в състояние да направи това с

0:44:32.599,0:44:34.809
0.5% грешка.

0:44:34.809,0:44:37.039
Така че може да го направи почти перфектно.

0:44:37.039,0:44:39.509
И така, обучихме първия си модел!

0:44:39.509,0:44:40.539
Нямаме идея как,

0:44:40.539,0:44:41.799
не знаем какво правим,

0:44:41.799,0:44:44.259
но наистина обучихме модел.

0:44:44.259,0:44:46.559
Това е добро начало.

0:44:46.559,0:44:51.309
И както виждате, можем да обучаваме модели доста бързо на един компютър,

0:44:51.309,0:44:55.089
каквито много може да получите безплатно.

0:44:55.089,0:45:00.869
Още нещо да отбележа, ако имате Mac... няма значение дали имате Windows,

0:45:00.869,0:45:05.069
Mac или Linux при използване на браузър,

0:45:05.069,0:45:11.470
но ако имате Mac, моля, не се опитвайте да използвате GPU-то.

0:45:11.470,0:45:16.150
Всъщност Mac и Apple дори вече не поддържат Nvidia GPU.

0:45:16.150,0:45:19.470
Така че това няма да е добър вариант

0:45:19.470,0:45:20.869
Така че придържайте се към Линукс.

0:45:20.869,0:45:25.010
Ще ви направи живота много по-лесен.

0:45:25.010,0:45:30.420
И така, първото нещо, което трябва да направим, е да го пробваме.

0:45:30.420,0:45:34.750
Така че, ако твърдя, че сме обучили модел, който може да подбира котки от кучета,

0:45:34.750,0:45:37.640
нека се уверим, че можем.

0:45:37.640,0:45:41.859
Вижте тази клетка.

0:45:41.859,0:45:42.859
Интересно е,

0:45:42.859,0:45:43.859
нали?

0:45:43.859,0:45:47.940
Създали сме widget.FileUpload() обект и сме го показали

0:45:47.940,0:45:50.690
и това ни показва бутон за кликване.

0:45:50.690,0:45:52.619
Както ви казах, нова е необикновен REPL,

0:45:52.619,0:45:55.319
можем дори да създаваме GUI (графичен потребителски интерфейс) в този REPL.

0:45:55.319,0:45:58.359
Ако кликна на този бутон,

0:45:58.359,0:46:00.170
мога да избера котка


0:46:04.130,0:46:11.230
и сега мога да превърна качените данни в изображение.

0:46:11.230,0:46:14.319
Има котка

0:46:14.319,0:46:22.510
и мога да изпълня predict (прогнозирай), и това е котка.

0:46:22.510,0:46:26.400
С вероятност 99.96%.

0:46:26.400,0:46:29.910
Виждате, току-що качихме снимка, която сме избрали.

0:46:29.910,0:46:30.930
Така че може да пробвате и вие.

0:46:30.930,0:46:31.930
Така ли е?

0:46:31.930,0:46:32.930
Вземете снимка на котка.

0:46:32.930,0:46:35.579
Намерете от интернет или идете и направете сами снимка

0:46:35.579,0:46:38.910
и не забравяйте да получите снимка на котка.

0:46:38.910,0:46:43.520
Това е нещо, което може да разпознава снимки на котки, а не рисунки на котки.

0:46:43.520,0:46:46.940
И както ще видим по време на курса,

0:46:46.940,0:46:52.050
този вид модели могат да научат само вида информация, която сте им дали

0:46:52.050,0:46:57.130
и до този момент сме дали, както ще видите, само снимки на котки.

0:46:57.130,0:47:06.700
Не анимационни котки, не нарисувани котки, не абстрактни изображения на котки, а просто снимки.

0:47:06.700,0:47:11.470
Сега ще погледнем какво всъщност е станало тук?

0:47:11.470,0:47:15.930
Както ще видите след момент, тук не получаваме добра информация. 

0:47:15.930,0:47:26.259
Ако видите това във вашата тетрадка, трябва да идете до: file, trust notebook.

0:47:26.259,0:47:30.559
И това казва на Jupyter, че е позволено да изпълнява кода, необходим за визуализиране на неща,

0:47:30.559,0:47:33.509
за да е сигурно, че няма проблем със сигурността.

0:47:33.509,0:47:35.880
И така, вече ще видите резултата.

0:47:35.880,0:47:39.880
Понякога ще виждате странен код като този.

0:47:39.880,0:47:43.609
Това е код, който създава изходни резултати.

0:47:43.609,0:47:46.349
Понякога скриваме този код,

0:47:46.349,0:47:47.800
понякога го показваме

0:47:47.800,0:47:51.940
и казано най-общо, може просто да игнорирате такива неща и да се съсредоточите върху това, което следва

0:47:51.940,0:47:52.940
като изход.

0:47:52.940,0:47:54.300
Няма да минавам през тези неща.

0:47:54.300,0:48:00.660
Вместо това ще разгледаме същите неща от слайдовете.

0:48:00.660,0:48:04.710
И така, това, което правим, е машинно самообучение,

0:48:04.710,0:48:07.750
Deep Learning е вид машинно самообучение.

0:48:07.750,0:48:09.170
Какво е машинно самообучение?

0:48:09.170,0:48:16.070
Машинното самообучение е просто, както и стандартното програмиране, то е начин да се накарат компютрите да правят нещо.

0:48:16.070,0:48:22.250
Но в случая е доста трудно да разбереш как да използваш нормално програмиране, за да разпознаеш

0:48:22.250,0:48:24.000
снимки на кучета от снимки на котки.

0:48:24.000,0:48:28.319
Как да създадеш циклите, да присвоиш променливите, условните конструкции,

0:48:28.319,0:48:31.789
необходими за създаване на програма, която разпознава кучета от котки на снимки.

0:48:31.789,0:48:33.190
Това е страшно сложно

0:48:33.190,0:48:34.589
Много, много сложно.

0:48:34.589,0:48:41.420
Толкова трудно, че до ерата на Deep Learning никой наистина нямаше модел, който да е донякъде точен

0:48:41.420,0:48:43.910
при тази очевидно лесна задача.

0:48:43.910,0:48:46.970
Защото не можем да напишем необходимите стъпки.

0:48:46.970,0:48:51.369
Обикновено пишем функция, която приема някакви входни данни, преминава през 

0:48:51.369,0:48:52.369
нашата програма

0:48:52.369,0:48:55.569
и дава някакви резултати.

0:48:55.569,0:49:02.530
Тази обща идея, при която програмата е нещо, което ние пишем (стъпките),

0:49:02.530,0:49:06.970
изглежда не работи добре за неща, като разпознаване на снимки.

0:49:06.970,0:49:12.470
Така през 1949 г. някой на име Артър Самуел започва да се опитва да измисли начин за решаване

0:49:12.470,0:49:16.030
на проблеми като разпознаване на снимки на котки и кучета.

0:49:16.030,0:49:23.000
И през 1962 г. той описа начин за това.

0:49:23.000,0:49:26.270
Най-напред той описа проблема: „Програмирането на компютър 

0:49:26.270,0:49:31.070
за този вид изчисления е в най-добрия случай трудна задача,

0:49:31.070,0:49:37.589
поради необходимостта да се изписва всяка незначителна стъпка от процеса в мъчителни подробности.

0:49:37.589,0:49:42.290
Компютрите са гигантски дебили, което всички ние, програмистите, напълно го признаваме."

0:49:42.290,0:49:46.769
Така че той казва, нека не казваме на компютъра точните стъпки, а да му дадем примери 

0:49:46.769,0:49:50.460
от проблема за решаване и да намери сам как да го реши.

0:49:50.460,0:49:56.269
И така, до 1961 г. той е изградил програма за шашки, която е победила щатския шампион в Кънектикът

0:49:56.269,0:50:03.450
не като указва стъпките, които да се предприемат, за да се играят шашки, а като прави това, 

0:50:03.450,0:50:09.990
което е "организиране на автоматични начини за проверка на ефективността на присвоените тегла

0:50:09.990,0:50:15.690
от гледна точка на реално постигнатия резултат и на механизъм за промяна на присвоените тегла, така че да се 

0:50:15.690,0:50:19.019
максимизират резултатите".

0:50:19.019,0:50:21.680
Това е ключовото изречение.

0:50:21.680,0:50:24.440
И то е доста сложно изречение, така че можете да му отделите известно време.

0:50:24.440,0:50:32.200
Основната идея е следната: вместо да казваме входни данни (inputs) за програмата и след това изходни резултати (outputs)

0:50:32.200,0:50:36.430
нека имаме входни данни - и да казваме на програмата модел -

0:50:36.430,0:50:38.140
имаме същата основна идея

0:50:38.140,0:50:40.349
вход - модел - резултати

0:50:40.349,0:50:43.760
и след това ще имаме второ нещо, наречено тегла.

0:50:43.760,0:50:50.569
И така, основната идея е, че този модел е нещо, което създава резултати не само на база,

0:50:50.569,0:50:58.410
в случая на състоянието на таблото с шашки, но и на база на някакво множество от тегла или параметри,

0:50:58.410,0:51:02.320
които описват как ще работи моделът.

0:51:02.320,0:51:09.039
Идеята е, ако можем да изредим всички възможни начини за игра на шашки

0:51:09.039,0:51:14.130
и след това опишем всеки един от тези начини, използвайки някакво множество от параметри, или както Самуел

0:51:14.130,0:51:15.830
ги нарича, тегла.

0:51:15.830,0:51:21.690
След това, ако имаме начин да проверим колко ефективно е текущото присвояване на тегла от гледна точка

0:51:21.690,0:51:27.549
на резултата, с други думи, дали конкретната от изредените стратегии за игра

0:51:27.549,0:51:33.140
на шашки приключва с победа или загуба на играта, и след това начин за промяна на теглата,

0:51:33.140,0:51:35.599
така че да се максимизират резултатите.

0:51:35.599,0:51:40.710
Тогава нека да опитаме да увеличим или намалим всяко едно от тези тегла едно по едно,

0:51:40.710,0:51:45.190
за да видим дали има леко по-добър начин за игра на шашки, и след това да го повторим същото

0:51:45.190,0:51:51.950
много, много пъти, и евентуално такава процедура може да се направи изцяло автоматична, и тогава

0:51:51.950,0:51:58.030
така програмирана машина ще може да учи от своя опит, така че този кратък параграф

0:51:58.030,0:52:00.200
е нещото.

0:52:00.200,0:52:05.650
Това е машинно самообучение, начин за създаване на програми, при който те учат,

0:52:05.650,0:52:08.000
вместо да се програмират.

0:52:11.349,0:52:16.930
Ако имаме такова нещо, тогава в общи линии ще имаме нещо, което изглежда така:

0:52:16.930,0:52:22.549
имате входни данни и тегловни коефициенти, подавани към модела, който създава резултати, например печелите

0:52:22.549,0:52:26.769
или губите, и мярка за качеството на резултата.

0:52:26.769,0:52:30.349
Припомнете си, това беше тази ключова стъпка и след това втората ключова стъпка е начин за актуализиране 

0:52:30.349,0:52:35.609
на теглата на база измереното качество на резултата и след това може да направите цикъл през този процес

0:52:35.609,0:52:43.450
и да създадете и обучите модел за машинно самообучение, така че това е абстрактната идея.

0:52:43.450,0:52:49.260
След като сме тренирали модела известно време, ще ни дава комплект тегловни коефициенти, който е доста добър;

0:52:49.260,0:52:55.089
вече можем да забравим начина, по който е бил обучен, и имаме нещо, което е

0:52:55.089,0:53:02.359
точно като това, освен че думата програма сега е заменена с думата модел.

0:53:02.359,0:53:07.239
И така, обучен модел може да се използва също като всяка друга компютърна програма.

0:53:07.239,0:53:13.509
Идеята е, че създаваме компютърна програма не като поставяме необходимите стъпки

0:53:13.509,0:53:19.619
за изпълнение на задачата, а като я обучаваме да научи да изпълнява задачата, в края на което пак имаме просто

0:53:19.619,0:53:26.759
друга програма. Това е така нареченият логически извод (inference), да се използва обучен модел

0:53:26.759,0:53:37.980
като програма, за да изпълнява задачи, например игра на шашки. И така, машинното самообучение е обучаване на програми, разработени,

0:53:37.980,0:53:43.309
като се дава възможност на компютъра да се учи от опита си, вместо ръчно да се програмират отделните стъпки.

0:53:43.309,0:53:53.640
Как бихте направили това за разпознаване на изображение, какъв е този модел и този набор от тегла,

0:53:53.640,0:53:59.700
такива, че като ги променяме, да получаваме все по-добро разпознаване на котки спрямо кучета.

0:53:59.700,0:54:02.210
Имам предвид, че за шашките

0:54:02.210,0:54:06.780
не е толкова трудно да си представим как може един вид да изброите, в зависимост от различни

0:54:06.780,0:54:11.289
видове „колко далеч е парчето на противника“, "какво трябва да направите

0:54:11.289,0:54:12.289
в тази ситуация",

0:54:12.289,0:54:16.079
"как трябва да претеглиш защитните спрямо агресивните стратегии" и други.

0:54:16.079,0:54:20.410
Изобщо не е очевидно, как трябва да се направи това при разпознаване на изображения.

0:54:20.410,0:54:29.240
Затова това, от което се нуждаем, е някаква функция за тук, която да е толкова гъвкава, че има

0:54:29.240,0:54:33.210
множество тегла, които могат да я накарат да направи всичко.

0:54:33.210,0:54:40.059
Най-гъвкавата възможна функция в света. Оказва се, че има такова 

0:54:40.059,0:54:41.059
нещо.

0:54:41.059,0:54:44.140
Това е невронната мрежа.

0:54:44.140,0:54:50.329
Ще опишем точно каква е тази математическа функция в следващите уроци.

0:54:50.329,0:54:56.160
За да я използваме реално, няма значение каква математическа функция е тя.

0:54:56.160,0:55:03.631
Това е функция, която, казваме, е „параметризирана“
под някакъв набор от тегла, с което имам предвид,

0:55:03.631,0:55:12.259
че като давам различни набори тегла, тя изпълнява различни задачи и на практика може да изпълни всяка

0:55:12.259,0:55:17.970
възможна задача. Така наречената теорема за универсалното приближение ни казва, че математически доказуемо 

0:55:17.970,0:55:26.319
тази функция може да реши всеки проблем, който е решим, с произволно ниво на точност.

0:55:26.319,0:55:28.589
Само ако намерите правилния набор от тегловни коефициенти.

0:55:28.589,0:55:33.210
Което е вид преформулиране на описаното по-рано в това как се справяме с

0:55:33.210,0:55:39.700
проблема на Мински (Марвин Мински). И така, невронните мрежи са толкова гъвкави, че ако може

0:55:39.700,0:55:44.609
да намерите правилния набор от коефициенти, те могат да решат всеки проблем, включително "Това котка ли е

0:55:44.609,0:55:46.289
или е куче?"

0:55:46.289,0:55:51.130
Така че това означава, че трябва да насочите усилията си към процеса на обучение, т.е. намиране

0:55:51.130,0:55:57.010
на добри тегла или присвояване на добри тегла по терминологията на Самуел. 

0:55:57.010,0:55:59.770
И как го правите това?

0:55:59.770,0:56:09.239
Искаме напълно общ начин да правим това, да променяме теглата въз основа на някаква мярка за

0:56:09.239,0:56:14.200
качеството на работа, като например колко е добър в разпознаване на котки спрямо кучета.

0:56:14.200,0:56:16.839
За наше щастие, оказва се, че такова нещо съществува!

0:56:16.839,0:56:21.539
И това нещо се нарича stochastic gradient descent или SGD (стохастично градиентно спускане).

0:56:21.539,0:56:26.540
Отново ще видим точно как това работи, ще го създадем сами от нулата, но

0:56:26.540,0:56:28.529
засега няма нужда да се тревожим за това.

0:56:28.529,0:56:34.210
Само ще ви кажа, че и SGD, и невронните мрежи изобщо не са 

0:56:34.210,0:56:35.210
математически сложни.

0:56:35.210,0:56:38.829
Те са почти изцяло сумирания и умножения.

0:56:38.829,0:56:45.109
Номерът е, че имаме много от тях - милиарди от тях, много повече, отколкото интуитивно можем

0:56:45.109,0:56:46.109
да възприемем.

0:56:46.109,0:56:52.940
Могат да правят изключителни неща, но не са ракетна наука.

0:56:52.940,0:56:58.609
Не са сложни неща и ще видим точно как работят.

0:56:58.609,0:57:03.049
Това е версията на Артур Самуел.

0:57:03.049,0:57:08.580
В наши дни не използваме точно същата терминология, но използваме съвсем същата идея.

0:57:08.580,0:57:12.660
Функцията, която се намира по средата,

0:57:12.660,0:57:14.549
наричаме я архитектура.

0:57:14.549,0:57:20.779
Архитектура е функцията, чиито коефициенти настройваме, за да я накараме да върши нещо.

0:57:20.779,0:57:24.190
Това е архитектурата, това е функционалната форма на модела.

0:57:24.190,0:57:28.849
Понякога хората казват модел и имат предвид архитектура, така че не позволявайте това да ви обърка особено.

0:57:28.849,0:57:30.559
Но правилната дума е архитектура.

0:57:30.559,0:57:34.619
Не ги наричаме тегла (тегловни коефициенти), наричаме ги параметри.

0:57:34.619,0:57:40.410
Теглата имат специално значение, това са особен вид параметри.

0:57:40.410,0:57:46.609
Нещата, които излизат от модела, архитектурата с параметрите, наричаме ги

0:57:46.609,0:57:49.809
прогнози (predictions).

0:57:49.809,0:57:55.660
Прогнозите се базират на два вида входна информация (inputs): независимите променливи, това са данните,

0:57:55.660,0:58:03.309
като снимките на котки и кучета, и зависими променливи, още известни като етикети (labels),

0:58:03.309,0:58:07.400
което е нещото, казващо ни "това е котка", "това е куче", "това е

0:58:07.400,0:58:08.400
котка".

0:58:08.400,0:58:09.779
Това ви е входът.

0:58:09.779,0:58:12.769
Резултатите са прогнозите.

0:58:12.769,0:58:18.670
Мярката за качество на работа, използвайки думите на Артур Самуел, е известна като загуба (loss).

0:58:18.670,0:58:24.020
Загубата се изчислява от етикетите и от прогнозите и след това има

0:58:24.020,0:58:26.720
актуализиране в обратна посока към параметрите.

0:58:26.720,0:58:33.210
Това е същата схема, която вече видяхме, но са сложени думите, които ще използваме

0:58:33.210,0:58:34.210
в наши дни.

0:58:34.210,0:58:40.039
И така, тази картина, ако сте забравили, това са параметрите, използвани

0:58:40.039,0:58:44.220
за тази архитектура да създаде модел, може да се върнете и да си припомните какво означават.

0:58:44.220,0:58:45.220
Какво са параметрите (parameters)?

0:58:45.220,0:58:46.500
Какво са прогнозите (predictions)?

0:58:46.500,0:58:47.500
Какво е загубата (loss)?

0:58:47.500,0:58:53.970
Загубата е някаква функция, коя измерва работата на модела по такъв начин, 

0:58:53.970,0:58:56.790
че можем да актуализираме параметрите.

0:58:56.790,0:59:06.170
Важно е да отбележим, че Deep Learning и машинното самообучение не са магия.

0:59:06.170,0:59:13.380
Модел може да се създаде само когато имате данни с примери с нещата, за които

0:59:13.380,0:59:14.869
се опитвате да научите. 

0:59:14.869,0:59:22.030
Моделът може да се обучи да работи само със закономерности, които сте видели във входните данни, използвани за обучението му.

0:59:22.030,0:59:27.109
Така че, ако нямаме скици на котки и кучета, тогава никога няма да има

0:59:27.109,0:59:32.519
актуализиране на параметрите, което ще направи архитектурата, по-точно архитектурата и

0:59:32.519,0:59:34.420
параметрите заедно представляват модела,

0:59:34.420,0:59:39.650
Та да кажем моделът, която ще направи модела по-добър в прогнозиране на скици на котки

0:59:39.650,0:59:43.799
и кучета, защото никога няма да има такива актуализации на теглата, защото никога не са получени

0:59:43.799,0:59:46.680
такива входни данни.

0:59:46.680,0:59:51.239
Забележете също така, че този подход за обучение създава единствено прогнози.

0:59:51.239,0:59:54.319
Не ви казва какво да правите с тях.

0:59:54.319,0:59:58.019
Това ще бъде много важно, когато мислим за неща, като система за даване на препоръки

0:59:58.019,1:00:01.230
от типа какъв продукт препоръчваме на някого.

1:00:01.230,1:00:04.950
Ами не знам, ние не правим това, нали?

1:00:04.950,1:00:09.759
Ние можем да предвидим какво ще каже някой за продукт, който сме му показали, но 

1:00:09.759,1:00:11.140
не създаваме действия.

1:00:11.140,1:00:12.310
Създаваме прогнози.

1:00:12.310,1:00:16.619
Това е супер важна разлика за разпознаване.

1:00:16.619,1:00:22.470
Не е достатъчно само да има примери за входни данни като снимки на кучета и котки.

1:00:22.470,1:00:26.309
Нищо не можем да направим без етикети.

1:00:26.309,1:00:31.359
Много често организациите казват: "Нямаме достатъчно данни."

1:00:31.359,1:00:35.150
В повечето случаи имат предвид: "Нямаме достатъчно категоризирани данни (labelled data)."

1:00:35.150,1:00:39.549
Защото, ако компанията се опитва да прави нещо с Deep Learning, често това е, защото 

1:00:39.549,1:00:43.180
се опитват да автоматизират или подобрят нещо, което вече правят.

1:00:43.180,1:00:48.420
Това означава, че по определение те имат данни за това нещо или имат начин за събиране на данни

1:00:48.420,1:00:49.420
за това нещо.

1:00:49.420,1:00:50.420
Защото те го правят,

1:00:50.420,1:00:51.420
нали?

1:00:51.420,1:00:55.089
Но често трудната част е поставянето на етикети.

1:00:55.089,1:00:57.569
Например в медицината,

1:00:57.569,1:01:00.789
ако се опитваш да създадеш модел за радиология.

1:01:00.789,1:01:06.579
Почти сигурно можете да получите много медицински снимки за почти всичко, за което се сетите.

1:01:06.579,1:01:11.769
Но може да е много трудно да ги маркирате според злокачествеността на тумор или според това

1:01:11.769,1:01:18.480
дали има менингиом или не, или каквото и да е, защото този вид етикети не са непременно

1:01:18.480,1:01:24.289
запазени по структуриран начин, поне в медицинската система на САЩ.

1:01:24.289,1:01:31.700
Така че това е важна разлика, която наистина се отразява на вашия вид стратегия.

1:01:31.700,1:01:40.000
След това, както видяхме от книгата PDP, моделът работи в някаква среда.

1:01:40.000,1:01:44.420
Пускате го и правите нещо с нещо.

1:01:44.420,1:01:50.640
И така, тази част от PDP рамката е изключително важна.

1:01:50.640,1:01:53.769
Имате модел, който всъщност прави нещо.

1:01:53.769,1:01:59.069
Например, изградили сте прогнозиращ модел на полицейския контрол, който предвижда, не препоръчва действия, а

1:01:59.069,1:02:02.460
предвижда къде ще бъде извършен арест.

1:02:02.460,1:02:06.670
Това е нещо, което много юрисдикции в САЩ използват.

1:02:06.670,1:02:10.809
Прогнозата е въз основа на данни и на базата на етикетирани данни.

1:02:10.809,1:02:20.779
И в този случай всъщност ще се използват
(в САЩ например) данни къде, ... мисля, че

1:02:20.779,1:02:25.099
в зависимост дали сте черен или бал, черните хора в САЩ, мисля, че са арестувани

1:02:25.099,1:02:31.480
около седем пъти по-често, примерно за притежание на марихуана, от белите,

1:02:31.480,1:02:37.579
въпреки че реалната консумация на марихуана е приблизително еднаква за двете

1:02:37.579,1:02:38.579
популации.

1:02:38.579,1:02:42.079
Така че, ако започнете с изместени данни (biased data) и изградите прогнозиращ модел на полицейския контрол,

1:02:42.079,1:02:49.430
прогнозите ще казват "ей, ще намерите някой за арестуване... тук!" на база

1:02:49.430,1:02:50.430
изкривени данни.

1:02:50.430,1:02:56.279
Затова служителите на реда могат да решат да съсредоточат полицейската си дейност върху зоните,

1:02:56.279,1:02:58.039
където се случват прогнозите.

1:02:58.039,1:03:01.940
В резултат ще намерят още хора за арестуване

1:03:01.940,1:03:05.430
и ще използват тези данни, за да захранят модела,

1:03:05.430,1:03:09.789
който сега ще установи: "ха, има още повече хора, които трябва да бъдат арестувани

1:03:09.789,1:03:12.640
в кварталите на черните", и това ще продължи.

1:03:12.640,1:03:17.000
Това е пример как моделът си взаимодейства със средата, за да създаде т.нар.

1:03:17.000,1:03:19.460
положителна обратна връзка,

1:03:19.460,1:03:23.930
при която колкото повече използваме модела, толкова по-изместени (изкривени - biased) стават данните, което прави модела дори още

1:03:23.930,1:03:26.440
по-изместен и така нататък.

1:03:26.440,1:03:32.299
И така, едно от нещата, с които трябва да сме особено внимателни при машинното самообучение, е да се разбере, 

1:03:32.299,1:03:38.009
как всъщност се използва този модел и какви неща могат да се случат в резултат

1:03:38.009,1:03:39.009
на това.

1:03:39.009,1:03:48.910
Само бих добавила, че това е пример за заместители (proxies), защото тук се използва арест

1:03:48.910,1:03:56.210
като заместител (proxy) за престъпление и мисля, че почти във всички случаи данните, които имате,

1:03:56.210,1:04:00.049
са заместител на някаква стойност, която реално ви интересува.

1:04:00.049,1:04:06.109
И разликата между заместителя и действителната стойност често може да се окаже значителна.

1:04:06.109,1:04:10.770
Благодаря, Рейчъл.

1:04:10.770,1:04:13.430
Това е наистина важен момент.

1:04:13.430,1:04:24.219
Добре, нека приключим, като разгледаме какво става в този програмен код.

1:04:24.219,1:04:34.880
Кода, който изпълняваме, е... един, два, три, четири, пет, шест... реда код.

1:04:34.880,1:04:39.829
Първият ред е ред за импортиране (import).

1:04:39.829,1:04:46.690
В Python не може да използвате външна библиотека, докато не импортирате от нея.

1:04:46.690,1:04:53.339
Обикновено в Python хората импортират само тези функции и класове, които им трябват

1:04:53.339,1:04:55.030
от библиотеката.

1:04:55.030,1:05:01.989
Но Python предлага и удобен начин, с който може да импортирате всичко от един модул, 

1:05:01.989,1:05:04.410
който да се постави *

1:05:04.410,1:05:07.029
В повечето случаи това е лоша идея,

1:05:07.029,1:05:12.479
защото, по подразбиране, начинът, по който работи Python, е, че ако въведете import *, той не 

1:05:12.479,1:05:16.640
въвежда само нещата, които са интересни и важни от библиотеката, 

1:05:16.640,1:05:18.520
от която опитвате да вземете нещо,

1:05:18.520,1:05:23.369
но въвежда и неща от всички библиотеки, които тази библиотека използва, и от всички библиотеки, които те използват,

1:05:23.369,1:05:27.190
и накрая приключвате, взривявайки пространството на имената по ужасни начини и предизвиквайки най-различни

1:05:27.190,1:05:28.760
дефекти (бъгове).

1:05:28.760,1:05:36.510
Понеже fastai е проектирана да се използва в тази PERL среда, където искате да има възможност

1:05:36.510,1:05:41.779
да правите голямо количество бързо прототипиране, ние изразходихме много време, за да измислим

1:05:41.779,1:05:45.410
как да избегнем този проблем, така че да може да импортирате със звезда безопасно.

1:05:45.410,1:05:49.630
Дали ще правите така или не, зависи изцяло от вас.

1:05:49.630,1:05:56.609
Но това е гарантирано, че ако импортирате * от библиотека на fastai, тя е изрично

1:05:56.609,1:06:01.999
проектирана по начин, че да получите само тези части, които в действителност ви трябват.

1:06:01.999,1:06:05.799
Едно нещо, което трябва да се спомене, е във видеото, което виждате, нарича се "fastai2".

1:06:05.799,1:06:09.849
Това е така, защото записваме този видеоклип, използвайки предварителна версия.

1:06:09.849,1:06:19.049
До момента, в който гледате онлайн (MOOC) версията, двойката ще е изчезнала.

1:06:19.049,1:06:25.609
Друго за отбелязване е, че в момента, в който говоря, има четири основни предварително дефинирани приложения

1:06:25.609,1:06:31.099
във fastai - изображения (vision, компютърно зрение), текст (text), таблични (tabular) и съвместно филтриране (collaborative filtering).

1:06:31.099,1:06:35.130
Ще научим за всички тях и много повече.

1:06:35.130,1:06:41.989
За всяко едно, да кажем vision, може да импортирате от .all, един вид мета-модул,

1:06:41.989,1:06:42.989
предполагам така може да го наречем.

1:06:42.989,1:06:48.079
И това ще ви даде всички неща, от които се нуждаете за най-типичните приложения с изображения.

1:06:48.079,1:06:55.779
И така, ако използвате система REPL като Jupyter notebook, това ще ви предостави всички неща,

1:06:55.779,1:07:01.619
от които се нуждаете, без да се налага да се връщате назад и да го мислите.

1:07:01.619,1:07:06.559
Един от проблемите с това е, че много потребители на Python не...

1:07:06.559,1:07:12.450
Ако погледнат към нещо, като untar_data, те ще разберат откъде идва,

1:07:12.450,1:07:14.219
като погледнат на реда с import-ите.

1:07:14.219,1:07:15.970
Ако импортирате звезда, не може да правите повече така.

1:07:15.970,1:07:19.829
Добрата новина е, че в REPL това не ви е нужно.

1:07:19.829,1:07:27.450
Можете буквално да въведете символ, да натиснете SHIFT - ENTER и ще ви каже точно

1:07:27.450,1:07:28.670
откъде е дошъл.

1:07:28.670,1:07:29.940
Както виждате

1:07:29.940,1:07:33.510
и това е много удобно.

1:07:33.510,1:07:43.859
В този случай, например, за да направим изграждането на набора с данни, извикахме ImageDataLoaders.from_name_func.

1:07:43.859,1:07:51.170
Всъщност мога да извикам специалната функция doc, за да получа документацията,

1:07:51.170,1:07:57.349
и както виждате, тя ми казва точно всичко, което трябва да подам, какви са стойностите по подразбиране и

1:07:57.349,1:08:07.940
най-важното, не само какво прави, но "show in docs" ме прехвърля към пълната документация,

1:08:07.940,1:08:11.190
включваща и пример.

1:08:11.190,1:08:17.180
Всичко в документацията на fastai има пример и готиното е, че 

1:08:17.180,1:08:20.770
цялата документация е написана в Jupyter тетрадки.

1:08:20.770,1:08:25.540
Това означава, че можете да отворите тетрадката Jupyter за този документ и сами да изпълните

1:08:25.540,1:08:33.280
съответния ред програмен код, да го видите в работа и да разгледате резултатите, и т.н.

1:08:33.280,1:08:36.760
Също така в документацията ще откриете, че има куп уроци.

1:08:36.760,1:08:40.501
Например, ако погледнете урока за vision, той ще покрива много неща, но едно

1:08:40.501,1:08:44.910
от нещата е, както виждате в случая, почти същия вид неща,

1:08:44.910,1:08:47.710
каквито гледаме в урок 1.

1:08:47.710,1:08:54.300
Така че във fastai има много документация и да се възползвате от нея е доста добра идея.

1:08:54.300,1:08:59.310
Тя е напълно достъпна за търсене и както споменах, може би най-важното, всяка една от тези 

1:08:59.310,1:09:05.050
страници документация е също и напълно интерактивна Jupyter тетрадка.

1:09:05.050,1:09:13.839
И така, разглеждайки още от кода, първият ред след импортирането е нещо, което

1:09:13.839,1:09:14.910
използва untar_data.

1:09:14.910,1:09:20.020
Това ще свали набора с данни, ще го разкомпресира и ще го сложи на компютъра ви.

1:09:20.020,1:09:22.770
Ако вече е свален, няма да го свали повторно.

1:09:22.770,1:09:25.710
Ако е вече декомпресиран, няма да го декомпресира отново.

1:09:25.710,1:09:32.540
И както виждате, fastai има предварително зададен достъп до редица много полезни набори с данни,

1:09:32.540,1:09:34.180
като този набор PETS с домашни любимци.

1:09:34.180,1:09:39.920
Данните са изключително важна част от Deep Learning, както може да си представите.

1:09:39.920,1:09:41.900
Ще виждаме много такива.

1:09:41.900,1:09:47.690
Създадени са от много герои, които са прекарали месеци или години,

1:09:47.690,1:09:51.260
събирайки данни, които можем да използваме, за да създаваме модели.

1:09:51.260,1:10:01.230
Следващата стъпка е да кажем на fastai какви са тези данни и това ще го изучаваме подробно.  

1:10:01.230,1:10:05.560
Но в случая ние основно казваме:
„Добре, съдържа изображения“ (Image...).

1:10:05.560,1:10:07.630
Съдържа изображения, които са на този път (path).

1:10:07.630,1:10:14.100
untar_data връща пътя, т.е. къде е бил декомпресиран наборът с данни

1:10:14.100,1:10:18.900
или ако е вече декомпресиран, ни казва къде е бил декомпресиран преди.

1:10:18.900,1:10:23.560
Трябва да й кажем неща, като какви изображения са на този път.

1:10:23.560,1:10:27.170
Едно от интересните неща е label_func.

1:10:27.170,1:10:33.600
Как да кажете за всеки файл дали е котка или куче?

1:10:33.600,1:10:37.330
И ако погледнете във файла със сведения (ReadMe) на оригиналния набор с данни, там се използва леко чудат

1:10:37.330,1:10:42.430
подход, който е, казват "о, всичко, където първата буква на името на файла е 

1:10:42.430,1:10:45.000
главна буква е котка".

1:10:45.000,1:10:46.430
Така са решили.

1:10:46.430,1:10:51.180
Затова тук просто създадохме малка функция, наречена is_cat, която връща първата буква,

1:10:51.180,1:10:52.480
дали е главна или не.

1:10:52.480,1:10:57.420
И казваме на fastai "така разбираш дали е котка".

1:10:57.420,1:11:01.260
Ще се върнем към тези две след малко.

1:11:01.260,1:11:04.640
И така, дотук разказахме какви са данните.

1:11:04.640,1:11:06.350
След това трябва да създадем нещо, наречено learner (изучаващ).

1:11:06.350,1:11:10.180
learner е нещо, което се учи, то прави обучението (training).

1:11:10.180,1:11:12.510
Трябва да му кажете какви данни да ползва.

1:11:12.510,1:11:16.570
След това трябва да му кажете каква архитектура да ползва.

1:11:16.570,1:11:19.810
Ще говоря много за това през курса.

1:11:19.810,1:11:25.460
Но в общи линии, съществуват много предварително зададени архитектури на невронни мрежи, които имат определени

1:11:25.460,1:11:26.660
предимства и недостатъци.

1:11:26.660,1:11:30.360
И за компютърното зрение (computer vision), архитектурата се нарича ResNet.

1:11:30.360,1:11:34.580
Просто много добра отправна точка и ние просто ще използваме една 

1:11:34.580,1:11:35.820
сравнително малка.

1:11:35.820,1:11:39.710
Така че всичко това е предварително дефинирано и настроено за вас.

1:11:39.710,1:11:43.380
И след това можете да кажете на fastai какви неща искате да се разпечатват, докато протича обучение.

1:11:43.380,1:11:47.730
В този случай казвате: "Кажете ни каква е грешката, моля, по време на обучение."

1:11:47.730,1:11:51.230
След това можем да извикаме особено важния метод, наречен fine_tune, за който ще учим

1:11:51.230,1:11:57.050
през следващия урок и който реално изпълнява обучението.

1:11:57.050,1:12:00.700
valid_pct прави нещо много важно -

1:12:00.700,1:12:08.160
отделя, в случая 20% от данните (пропорция 0.2), и не ги използва за обучение

1:12:08.160,1:12:09.230
на модела.

1:12:09.230,1:12:12.730
Вместо това той го използва, за да ви каже процента на грешки на модела.

1:12:12.730,1:12:19.890
И така, за определяне на тази мярка, процента грешки, при fastai винаги се използва отделената част от данните,

1:12:19.890,1:12:22.290
с която не е обучаван моделът.

1:12:22.290,1:12:27.040
Идеята е, за това ще говорим много в следващите уроци,

1:12:27.040,1:12:31.220
но основната идея е, че искаме да сме сигурни, че не получаваме прекомерно нагаждане (overfitting, преобучаване).

1:12:31.220,1:12:33.540
Нека обясня.

1:12:33.540,1:12:35.090
Прекомерното нагаждане (overfitting) изглежда така:

1:12:35.090,1:12:38.860
Да кажем, че се опитваме да направим функция, която съответства на всички тези точки.

1:12:38.860,1:12:44.050
Една добра функция ще изглежда така, нали?

1:12:44.050,1:12:47.940
Но може да подберете, даже може да напаснете много по-точно с тази функция.

1:12:47.940,1:12:50.970
Вижте, тя минава много по-близо до всички точки от другата.

1:12:50.970,1:12:53.240
Излиза, че това очевидно е по-добра функция.

1:12:53.240,1:13:00.010
С изключение на това, че щом излезете извън областта на точките, особено ако излезете от краищата, 

1:13:00.010,1:13:01.640
очевидно няма смисъл.

1:13:01.640,1:13:06.060
Ето това наричаме пренапасната (overfit) функция.

1:13:06.060,1:13:08.570
overfitting се получава поради най-различни причини -

1:13:08.570,1:13:11.970
използваме модел, който е прекалено голям или използваме недостатъчно данни.

1:13:11.970,1:13:14.510
Ще говорим за това.

1:13:14.510,1:13:22.370
Но в действителност занаятът Deep Learning в същината си е да се създаде модел, който е правилно 

1:13:22.370,1:13:23.370
напаснат (proper fit).

1:13:23.370,1:13:27.480
И единственият начин да разберете дали моделът е правилно напаснат, е да видите дали работи добре

1:13:27.480,1:13:31.620
с данни, които не са използвани за обучението му.

1:13:31.620,1:13:36.830
Затова винаги си оставяме настрана част от данните, за да създадем т.нар. набор за валидиране.

1:13:36.830,1:13:41.270
Наборът за валидиране са данните, които няма да пипаме изобщо при обучението 

1:13:41.270,1:13:49.230
на модела, а ще ги използваме само за да разберем дали моделът реално работи,

1:13:49.230,1:13:51.100
или не.

1:13:51.100,1:13:57.080
Едно нещо, което Sylvain спомена в книгата е..., едно от интересните неща 

1:13:57.080,1:14:04.500
при изучаването на fastai е, че научавате много интересни практики в програмирането.

1:14:04.500,1:14:11.000
Аз съм програмирал още от дете, значи към 40 години.

1:14:11.000,1:14:18.060
И двамата, Силвен и аз, работим наистина много усилено, за да накараме Python да направи много неща за нас,

1:14:18.060,1:14:22.960
използваме практики за програмиране, които ни правят много продуктивни и ни позволяват

1:14:22.960,1:14:27.120
да се върнем към кода си след години и пак да го разбираме.

1:14:27.120,1:14:34.700
И така, ще видите в кода ни, че често правим неща, които може да не сте виждали преди.

1:14:34.700,1:14:39.260
Много студенти, които са минали през предишните ни курсове, казват, че са научили

1:14:39.260,1:14:44.100
много за програмирането, програмирането на Python и за софтуерното инженерство от този курс.

1:14:44.100,1:14:49.350
Така че, проверете, когато видите нещо ново, проверете го и не се колебайте да питате във форумите,

1:14:49.350,1:14:53.410
ако сте любопитни, защо нещо е направено по този начин.

1:14:53.410,1:14:59.250
Едно нещо, което трябва да отбележа, е точно както споменах, че import * е нещо, което повечето програмисти на Python

1:14:59.250,1:15:05.890
не правят, защото повечето библиотеки не го поддържат правилно,

1:15:05.890,1:15:07.130
ние правим много неща от този род.

1:15:07.130,1:15:11.240
Правим много неща, при които не следваме традиционния подход за програмиране на Python.

1:15:11.240,1:15:19.940
Тъй като съм използвал толкова много езици през годините, кодирам не по начин, който е конкретно 

1:15:19.940,1:15:23.940
Pythonic, но включва идеи от много други езици и много други

1:15:23.940,1:15:31.230
обозначения, и силно адаптира нашия подход към програмирането на Python въз основа на това, което работи

1:15:31.230,1:15:34.000
добре за науката за данните (data science).

1:15:34.000,1:15:40.750
Това означава, че кодът, който виждате във fastai, вероятно няма да отговаря на 

1:15:40.750,1:15:45.600
указанията за стил и нормалните подходи на работното ви място, ако там ползвате Python.

1:15:45.600,1:15:53.440
Очевидно ще трябва да се убедите, че съответствате на правилата за програмиране на вашата организация,

1:15:53.440,1:15:56.420
а не да следвате нашите.

1:15:56.420,1:16:00.910
Но може би в своето хоби ще можете да следвате нашите и да видите, дали ще ги намерите за интересни

1:16:00.910,1:16:05.270
и полезни, или дори да експериментирате с това във вашата компания, ако сте мениджър

1:16:05.270,1:16:08.480
и се интересувате от това.

1:16:08.480,1:16:17.001
И така, за да завършим, ще ви покажа нещо доста интересно, което е,

1:16:17.001,1:16:27.580
погледнете този код - untar_data, ImageDataLoaders.from_name_func, learner, fine_tune.

1:16:27.580,1:16:33.740
untar_data, SegmentationDataLoaders.from_label_func, learner, fine_tune.

1:16:33.740,1:16:39.070
Почти същият код и това създава модел, който върши нещо, ха, напълно различно!

1:16:39.070,1:16:42.520
Това е нещо, което е взело изображения,

1:16:42.520,1:16:45.420
това вляво са маркираните данни (labelled data).

1:16:45.420,1:16:51.850
Имаме изображение с цветови код, който отбелязва дали има кола, дърво или сграда,

1:16:51.850,1:16:54.201
или небе, или осева линия, или път.

1:16:54.201,1:16:59.410
Отдясно е нашият модел и нашият модел успешно е намерил за всеки пиксел

1:16:59.410,1:17:02.960
дали е кола, пътна линия, или път.

1:17:02.960,1:17:06.520
Направило го е за под 20 секунди, така ли е?

1:17:06.520,1:17:08.480
Така че, това е много бърз модел.

1:17:08.480,1:17:13.060
Направил е някои грешки, например, изпуснал е тази линия от маркировката и част от колите,

1:17:13.060,1:17:15.710
мисли, че са къщи, 

1:17:15.710,1:17:21.400
но може да видите, че ако обучавате модела още няколко минути, ще е почти перфектен.

1:17:21.400,1:17:26.300
Но може да видите основната идея, че можем много бързо с почти

1:17:26.300,1:17:31.890
същия код да създадем нещо, което не категоризира котки и кучета, а прави т.нар сегментация:

1:17:31.890,1:17:34.950
определя какво е всеки пиксел от изображението.

1:17:34.950,1:17:36.980
Вижте пак същото:

1:17:36.980,1:17:39.520
from ... import * TextDataLoader.from_folder

1:17:39.520,1:17:42.530
learner learn fine_tune

1:17:42.530,1:17:43.830
Същият основен код.

1:17:43.830,1:17:49.250
Това сега е нещо, където можем да подадем изречение и то ще разбере дали

1:17:49.250,1:17:55.380
изразява положителна или отрицателна емоция, при това ни дава точност 93%

1:17:55.380,1:18:04.440
за тази задача, след около 15 минути с IMDb набора данни, който съдържа хиляди пълноразмерни

1:18:04.440,1:18:08.960
рецензии на филми (1000 до 3000 думи на рецензия).

1:18:08.960,1:18:13.500
Числото, което получихме със същите три реда код, щеше да е най-доброто

1:18:13.500,1:18:19.960
на света за тази задача, с този много популярен сред академичните среди набор данни, примерно за 2015 г.,

1:18:19.960,1:18:21.670
предполагам
.
1:18:21.670,1:18:31.700
Така че създаваме модел от световно ниво в нашия браузър, използвайки някакъв базов код.

1:18:31.700,1:18:33.390
Ето същите основни стъпки отново:

1:18:33.390,1:18:35.110
from import * 
untar_data

1:18:35.110,1:18:38.270
TabularDataLoaders.from_csv

1:18:38.270,1:18:39.970
learner fit

1:18:39.970,1:18:52.030
Сега се изгражда модел, който предвижда заплата въз основа на csv таблица, съдържаща тези

1:18:52.030,1:18:53.030
колони.

1:18:53.030,1:18:56.620
И така, това са таблични данни (tabular data).

1:18:56.620,1:18:57.620
Ето същите основни стъпки:

1:18:57.620,1:18:59.600
from import *
untar_data

1:18:59.600,1:19:02.700
CollabDataLoaders.from_csv learner fine_tune

1:19:02.700,1:19:10.850
Това вече изгражда нещо, което предвижда за всяка комбинация от потребител и филм,

1:19:10.850,1:19:17.430
каква оценка смятаме, че потребителят ще даде на този филм въз основа на други филми, които

1:19:17.430,1:19:18.880
са гледали и харесвали в миналото.

1:19:18.880,1:19:23.230
Това се нарича съвместно филтриране (collaborative filtering) и се ползва в системите за препоръки.

1:19:23.230,1:19:29.520
И така, видяхте малко примери за всяко от четирите приложения във fastai.

1:19:29.520,1:19:34.420
Както ще видите през този курс, един и същи основен код, както и едни и същи основни

1:19:34.420,1:19:41.700
математически и програмни концепции, ни позволяват да правим много различни неща,

1:19:41.700,1:19:43.980
използвайки един и същи основен подход.

1:19:43.980,1:19:47.180
И това е заради Артур Самуил,

1:19:47.180,1:19:56.840
заради неговото фундаментално описание на това, което можеш да направиш, ако имаш начин да

1:19:56.840,1:20:01.630
параметризираш модел и ако имаш процедура за актуализация, която може да актуализира теглата,

1:20:01.630,1:20:09.850
за да подобри функцията на загубите, и в този случай за модел можем да използваме невронна мрежа, която е

1:20:09.850,1:20:14.350
изключително гъвкава функция.

1:20:14.350,1:20:18.250
Това е за този първи урок!

1:20:18.250,1:20:23.221
Той е малко по-кратък, отколкото ще бъдат другите уроци, като причината за това е, че

1:20:23.221,1:20:29.850
сме, както споменах, в началото на глобална пандемия, тук или поне на Запад,

1:20:29.850,1:20:32.710
други държави са много по-напред.

1:20:32.710,1:20:36.520
Затова прекарахме известно време да говорим за това в началото на курса. Може да намерите това

1:20:36.520,1:20:39.500
видео отделно.

1:20:39.500,1:20:45.730
През следващите уроци ще има още Deep Learning.

1:20:45.730,1:20:53.780
Това, което предлагам да правите през следващата седмица, преди да работите по следващия урок, е 

1:20:53.780,1:20:58.470
да се уверите, че може да пуснете GPU сървър, че може да го спрете, когато приключите, и че

1:20:58.470,1:21:06.010
може да изпълните целия код оттук и като минавате през него, да видите дали се използва Python

1:21:06.010,1:21:14.750
по начин, който познавате; използвайте документацията, използвайте doc-функцията, направете търсене

1:21:14.750,1:21:20.480
в документацията на fastai, вижте какво прави, вижте дали можете да вземете самите тетрадки от документацията

1:21:20.480,1:21:22.270
и да ги изпълните.

1:21:22.270,1:21:26.090
Просто опитайте да се почувствате комфортно, да се ориентирате в обстановката,

1:21:26.090,1:21:30.340
тъй като най-важното нещо за този стил на обучение, това обучение отгоре надолу, 

1:21:30.340,1:21:34.710
е да можете да провеждате експерименти и това означава, че трябва да можете да изпълнявате код.

1:21:34.710,1:21:41.360
Затова моята препоръка е: не продължавайте, докато не изпълните кода, прочетете главата

1:21:41.360,1:21:47.850
от книгата и след това преминете въпросника.

1:21:47.850,1:21:52.980
Предстои ни още работа за наборите с данни за валидиране и за тестване и за обучението с пренос (transfer learning).

1:21:52.980,1:21:58.360
Така че все още няма да можете да го направите целия, но се опитайте да изпълните всички части, които можете, въз основа

1:21:58.360,1:22:01.020
на видяното от курса досега.

1:22:01.020,1:22:04.130
Рейчъл, има ли нещо, което искаш да добавиш, преди да тръгнем?

1:22:04.130,1:22:08.350
Добре, много благодаря на всички, че се присъединихте към нас за урок първи, и наистина ще се радваме

1:22:08.350,1:22:14.880
да се видим следващия път, където ще научим за обучението с пренос и тогава

1:22:14.880,1:22:22.150
ще продължим със създаването на реална работна (production) версия на приложението, която ще можем

1:22:22.150,1:22:27.500
да сложим в интернет, и ще може да започнете да създавате приложения, които може да покажете на приятелите си

1:22:27.500,1:22:29.960
и те да започнат да си играят с тях.

1:22:29.960,1:22:30.440
Чао на всички!

